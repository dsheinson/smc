\documentclass{article}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}

\begin{document}

\section{Model} \label{sec:model}

We can represent a DLM as a state space model with observation and state equations given by

\begin{align}
y_t &= Fx_t + v_t \label{eqn:obs} \\
x_t &= Gx_{t-1} + w_t \label{eqn:state}
\end{align}

\noindent where $v_t$ and $w_t$ are iid Gaussian random variables with mean 0 and covariance matrices $V$ ($q$ by $q$) and $W$ ($p$ by $p$) , respectively. We also assume $v_t \perp w_{t'}$ for all $t$ and $t'$. $y_t$ ($q$ by $1$) is the observed data at time $t$, and $x_t$ ($p$ by $1$)  is the unknown state at time $t$. We regard $F$ ($q$ by $p$) and $G$ ($p$ by $p$) as known, and consider the special case where $V = \sigma^2\tilde{V}$ and $W = \sigma^2\tilde{W}$, where $\tilde{V}$ and $\tilde{W}$ are known and $\sigma^2$ is an unknown parameter. A fully Bayesian model is specified by defining the prior $p(x_0,\sigma^2)$, which we give as \[\sigma^2 \sim \mbox{IG}(a_0,b_0) \qquad x_0|\sigma^2 \sim \mbox{N}(m_0,\sigma^2\tilde{C}_0)\]
\noindent where $\alpha_0$, $\beta_0$, $m_0$, and $\tilde{C}_0$ are known, $\mbox{IG}(a,b)$ represents the inverse-gamma distribution with shape parameter $a$ and rate parameter $b$, and $N(\mu,\sigma^2)$ represents the normal distribution with mean $\mu$ and variance $\sigma^2$. We could also define the model in terms of the precision, $\phi = 1 / \sigma^2$, and specify the prior $p(x_0,\phi)$ by \[\phi \sim \mbox{G}(\alpha_0,\beta_0) \qquad x_0|\phi \sim \mbox{N}(m_0,\phi^{-1}\tilde{C}_0)\]
\noindent where $G(a,b)$ represents the gamma distribution with shape parameter $a$ and rate parameter $b$.

\section{Estimation} \label{sec:estimation}

Let $y_{1:t} = (y_1,y_2,\ldots,y_t)'$ represent the vector of data observed up until time $t$, $x_{0:t} = (x_0,x_1,x_2,\ldots,x_t)'$ the vector of unknown states up until time $t$, and let $\theta$ denote the vector of any unknown fixed parameters in the model. In general, the posterior distribution $p(x_t,\theta|y_{1:t})$ is not analytically tractable for models of the form given in \eqref{eqn:obs} and \eqref{eqn:state}. However, we can calculate this distribution for the special case described in Section \ref{sec:model}.

We can update the calculation of $p(x_t,\theta|y_{1:t})$ as data arrives sequentially using recursive equations. Denote the prior distribution $p(x_0, \phi)$ defined in Section \ref{sec:model} by $(x_0, \phi) \sim \mbox{NG}(m_0, \tilde{C}_0, \alpha_0, \beta_0)$. Given \[(x_{t-1},\phi)|y_{1:t-1} \sim \mbox{NG}(m_{t-1},\tilde{C}_{t-1}, \alpha_{t-1}, \beta_{t-1})\]
\noindent for $t \ge 1$, the posterior distribution $p(x_t,\phi|y_{1:t})$ is $\mbox{NG}(m_t,\tilde{C}_t,\alpha_t,\beta_t)$ where $m_t$, $\tilde{C}_t$, $\alpha_t$, and $\beta_t$ are calculated through the following equations \citep{petris2009dynamic}:

\begin{align}
a_t &= Gm_{t-1} &\qquad \tilde{R}_t &= G\tilde{C}_{t-1}G + W \label{eqn:state.pred} \\
f_t &= Fa_t &\qquad \tilde{Q_t} &= F\tilde{R}_tF + V \label{eqn:obs.pred} \\
m_t &= a_t + \tilde{R}_tF\tilde{Q}_t^{-1}(y_t-f_t) &\qquad \tilde{C}_t &= \tilde{R}_t - \tilde{R}_tF'\tilde{Q}_t^{-1}F\tilde{R}_t' \\
\alpha_t &= \alpha_{t-1} + q / 2 &\qquad \beta_t &= \beta_{t-1} + (y_t-f_t)'\tilde{Q}_t^{-1}(y_t-f_t)
\end{align}

\noindent where $q$ is the dimension of $y_t$.

We can also calculate the marginal posterior distributions $p(x_t|y_{1:t})$ and $p(\phi|y_{1:t})$, given by

\begin{equation}
x_t|y_{1:t} \sim \mbox{T}(m_t,\tilde{C}_t \frac{\beta_t}{\alpha_t},2\alpha_t) \qquad \phi|y_{1:t} \sim \mbox{G}(\alpha_t,\beta_t) \label{eqn:margpost}
\end{equation}

In addition, we can calculate the one-step ahead predictive density $p(y_t|y_{1:t-1})$ using equations \eqref{eqn:state.pred} and \eqref{eqn:obs.pred} since \[y_t|y_{1:t-1} \sim \mbox{T}(f_t,\tilde{Q}_t \frac{\beta_{t-1}}{\alpha_{t-1}},2\alpha_{t-1})\]
\noindent where $\mbox{T}(\mu,\Sigma,d)$ represents the non-standard student-t distribution with location parameter $\mu$, scale parameter $\Sigma$, and $d$ degrees of freedom.

\section{Resample-move algorithm} \label{sec:rm}

As mentioned in Section \ref{sec:estimation}, models of the form described by equations \eqref{eqn:obs} and \eqref{eqn:state} generally do not emit tractable forms of the posterior distributions $p(x_{0:t},\theta|y_{1:t})$ or $p(x_t,\theta|y_{1:t})$. Thus, in many cases we must turn toward methods for approximating these distributions such as particle filtering.

Using particle filtering, we can approximate the posterior distribution $p(x_{0:t},\theta|y_{1:t})$ through a weighted sample approximation, each sample being termed a \emph{particle}. Let $x^{(j)}_{0:t}$ be the vector of unknown states up until time $t$ for particle $j$, $\theta^{(j)}_t$ be the values of the fixed parameters at time $t$ for particle $j$, and $w^{(j)}_t$ be the weight of particle $j$ at time $t$. We require that $\sum_{j=1}^J w^{(j)}_t = 1$, where $J$ is the total number of particles.

We initialize the algorithm by drawing $\left(x^{(j)}_0, \theta^{(j)}_0\right)$ from some prior distribution $p(x_0,\theta_0)$ and setting $w^{(j)}_0 = 1 / J$ for $j = 1,\ldots,J$. Given a particle sample approximation of the posterior $p(x_{0:t-1},\theta|y_{1:t-1})$ - i.e. $\left(x^{(j)}_{0:t-1},\theta^{(j)}_{t-1}\right)$ with associated weights $w^{(j)}_{t-1}$ for $j=1,\ldots,J$ - we move to a particle sample approximation of $p(x_{0:t},\theta|y_{1:t})$ by the following steps:

\begin{enumerate}
\item Augmentation: For each $j = 1,\ldots,J$, draw $\tilde{x}^{(j)}_t$ from some state evolution distribution $p\left(x_t|x^{(j)}_{t-1},\theta^{(j)}_{t-1}\right)$. Add $\tilde{x}^{(j)}_t$ to particle $j$ and denote the new augmented particle by $\left(\tilde{x}^{(j)}_{0:t},\theta^{(j)}_{t-1}\right)$.
\item Update weights: For each $j = 1,\ldots,J$, calculate the \emph{incremental weight} $\alpha^{(j)}_t = p\left(y_t|\tilde{x}^{(j)}_t,\theta^{(j)}_{t-1}\right)$ and then the \emph{unnormalized weight} $\bar{w}^{(j)}_t = w^{(j)}_{t-1}\alpha^{(j)}_t$.
\item Renormalize weights: For each $j = 1,2,\ldots,J$, calculate the \emph{normalized weights}, $\tilde{w}^{(j)}_t$, by setting $\tilde{w}^{(j)}_t = \bar{w}^{(j)}_t / \sum_{l=1}^J \bar{w}^{(l)}_t$.
\item Resample (optional): For each $j$, sample an index $k_j$ from $\{1,2,\ldots,j,\ldots,J\}$ with associated probabilities $\{\tilde{w}^{(1)}_t,\tilde{w}^{(2)}_t,\ldots,\tilde{w}^{(j)}_t,\ldots,\tilde{w}^{(J)}_t\}$.
\item {\tt if( }resampling was performed{\tt )\{} \\
\begin{enumerate}[label=\alph*.]
\item Update weights: Set $w^{(j)}_t = 1 / J$ for all $j$.
\item Move particles: For each $j = 1,2,\ldots,J$, draw a new particle $\left(x^{(j)}_{0:t},\theta^{(j)}_t\right)$ from some transition kernel $q\left(x_{0:t},\theta|\tilde{x}^{(j)}_{0:t},\theta^{(j)}_{t-1}\right)$ with invariant distribution $p(x_{0:t},\theta|y_{1:t})$.
\end{enumerate}
{\tt \} else \{} For each $j = 1,2,\ldots,J$, set $w^{(j)}_t = \tilde{w}^{(j)}_t$ and $\left(x^{(j)}_{0:t},\theta^{(j)}_t\right) = \left(\tilde{x}^{(j)}_{0:t},\theta^{(j)}_{t-1}\right)$.{\tt\}}
\end{enumerate}

The `Resample' step is optional in the sense that if particle weights are not out of balance, it is not necessary to resample the particles because the weighted sample of particles would still provide a good approximation of the posterior. In this case, it would also not be necessary to move the particles, since the problem of degeneracy in the fixed parameters caused by resampling would not be present. By only resampling and moving particles at iterations of the particle filter for which it is necessary, we increase the efficiency of the algorithm. Measures of nonuniformity of particle weights such as effective sample size, coefficient of variation, and entropy can be used to determine when resampling should be performed \citep{Liu:Chen:Wong:reje:1998}. %We use an effective sample size threshold of 80\% of the total number of particles to determine when resample and move particles.

In addition, there are different ways to implement the `Resample' step that may be more efficient under certain conditions. The algorithm as written above implicity uses a \emph{multinomial} resampling scheme, i.e. when we say `sample an index $k_j$ ...'. Additional techniques for resampling exist including \emph{residual}, \emph{stratified}, and \emph{systematic} resampling. \citet{Douc:Capp:Moul:comp:2005} provides an overview of these methods. %We use stratified resampling.

\section{Model Comparison}

Using the algorithm described in Section \ref{sec:rm} we obtain an approximation to $p(\theta_t|y_{1:t})$ through $\sum_{j=1}^J w_{j,t}\delta_{\theta_{j,t}}(x)$, where $\delta_{x_0}(x)$ is the Dirac delta mass function centered at $x_0$. In addition we can use the weighted particle sample to obtain an approximation to the marginal likelihood $p(y_{1:t})$. We first note that given $p(y_{1:t-1})$, $p(y_{1:t})$ can be updated recursively by

\begin{equation}
p(y_{1:t}) = p(y_t|y_{1:t-1})p(y_{1:t-1}) \label{eqn:marglik}
\end{equation}

\noindent Next, we note that an approximation of $p(y_t|y_{1:t-1})$ can be obtained from the weighted particle sample by
\begin{equation}
p(y_t|y_{1:t-1}) \approx \sum_{j=1}^J w^{(j)}_{t-1}\alpha^{(j)}_t \label{eqn:condmarg}
\end{equation}

Having now prescribed a method for approximating $p(y_{1:t})$ sequentially within the resample-move algorithm, we can compare a set of possible models ${M_1,M_2,\ldots,M_p}$ by calculating posterior model probabilities by
\begin{equation}
p(M_i|y_{1:t}) = \frac{p(y_{1:t}|M_i)p(M_i)}{\sum_{i=1}^p p(y_{1:t}|M_i)p(M_i)} \label{eqn:modelcomp}
\end{equation}

\section{Example}

Consider three models of the form described in \eqref{eqn:obs} and \eqref{eqn:state}. For each model, we let $V = \sigma^2\tilde{V}$, $W = \sigma^2\tilde{W}$, and known values $F = G = \tilde{V} = 1$. We let $\tilde{W}$ be known at values 0.5, 1, and 2 for models $M_1$, $M_2$, and $M_3$, respectively. Given data $y_{1:T}$ that we suspect was generated according to one of these models and a set of prior model probabilities, posterior model probabilities can be calculated exactly using equations \eqref{eqn:marglik} and \eqref{eqn:modelcomp} since the predictive distribution $p(y_t|y_{1:t-1})$ is known in this case to follow a Student-t distribution. We can compare these values with those obtained from running the resample-move particle filter and then using equations \eqref{eqn:condmarg} and \eqref{eqn:modelcomp} to approximate the posterior model probabilities.

A single time series of data, $y_{1:T}$ with $T = 100$, was simulated from $M_2$ with common observation and state equation variance $\sigma^2$ set equal to 1. From this simulated data, we can calculate directly the marginal likelihood of observing this data under $M_2$ ($\log p(y_{1:200}) = -191.3302$). We can also calculate the marginal posterior distributions $p(x_t|y_{1:t})$, $p(\phi|y_{1:t})$, and one-step ahead predictive distributions $p(y_t|y_{1:t-1})$ for $t = 1,2,\ldots,200$. This allows us to compute 95\% credible intervals for the unknown states and parameter and 95\% one-step ahead prediction intervals for each data point, displayed in Figure \ref{fig:sim}.

\begin{figure}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=1.0\textwidth]{../graphs/cv-test-states-1}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=1.0\textwidth]{../graphs/cv-test-precision-1}
\end{minipage}
\caption{On left, sequential 95\% credible intervals (red lines) for $x_t|y_{1:t}$ and 95\% prediction intervals (blue lines) for $y_t|y_{1:t-1}$ (right) for $t = 1, 2, \ldots, 100$. On right, sequential 95\% credible intervals (red lines) for $\phi|y_{1:t}$ for $t = 1, 2, \ldots, 100$. Black horizontal lines indicate the true simulated states (left) the true precision (right) used for simulation ($\phi = 1$) . Black points in left picture represent simulated data points. Credible intervals were calculated analytically using the student-t distribution.} \label{fig:sim}
\end{figure}

To assess the variability of the approximation to the log marginal likelihood of the simulated data obtained using the resample-move particle filter, we ran the particle filter 20 times each using 100, 500, 1000, 5000, and 10000 particles. Figure \ref{fig:pf-hist} displays kernel density estimates of the distribution of log marginal likelihoods calculated from these particle filter runs, and Figure \ref{fig:pf-1sim-ternary} displays compositional plots of posterior model probabilities for each particle filter run compared with the true posterior model probabilities.

\begin{figure}
\includegraphics[width=1.0\textwidth]{../graphs/cv_pf_loglik-1}
\caption{Kernel density estimates of log marginal likelihoods of a single time series simulated from $M_2$, approximated assuming $M_2$ true using the resample-move particle filter, for each of 20 particle filter runs using 500 (red), 1000 (green), 5000 (turquoise), and 10000 (blue) particles.} \label{fig:pf-hist}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\textwidth]{../graphs/cv_pf_ternary-1}
\caption{Ternary compositional plots of posterior model probabilities of $M_1$ ($\tilde{W} = 0.5$), $M_2$ ($\tilde{W} = 1$), and $M_3$ ($\tilde{W} = 2$) approximated using the resample-move particle filter repeatedly on the same data set (black circles), compared with the true posterior model probabilities for that data set (red circle), for increasing number of particles (different plot panels). The probability of a model generating a time series of observations is visualized by each point's proximity to that model's corner of the triangle.} \label{fig:pf-1sim-ternary}
\end{figure}

Next, we simulated 19 more data sets from $M_2$ with $\sigma^2 = 1$ (making 20 total simulated data sets). For each data set, the log marginal likelihoods and posterior model probabilities under models $M_1$, $M_2$, and $M_3$ were calculated analytically. In addition, the resample-move particle filter was run with 100, 500, 1000, and 5000 particles under each of the three models for each simulated data set. Thus, for each time series of data, we can compare which model has the highest true posterior probability with the one that has highest posterior probability as approximated by the resample-move particle filter. Table \ref{tab:cont} displays these results for increasing number of particles used in the resample-move algorithm. If the particle filter approximation is in line with the true posterior, then all off-diagonal cells in the tables should be 0.

\begin{table}[h]
\centering
\caption{Contingency table of models with highest posterior model probability calculated analytically (rows) and approximated using the resample-move particle filter (columns) with increasing number of particles (tables).} \label{tab:cont}
\begin{tabular}{|c|c|c|c|}
  \hline
{\bf 100 Particles} & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
  \hline
$M_1$ &   1 &   1 &   0 \\
\hline
$M_2$ &   2 &   7 &   1 \\
\hline
$M_3$ &   0 &   4 &   4 \\
   \hline
\end{tabular}
\quad
\begin{tabular}{|c|c|c|c|}
  \hline
{\bf 500 Particles} & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
  \hline
$M_1$ &   2 &   0 &   0 \\
\hline
$M_2$ &   1 &   7 &   2 \\
\hline
$M_3$ &   0 &   2 &   6 \\
   \hline
\end{tabular}
%\end{table}
\quad
%\begin{table}[h]
%\centering
%\caption{Contingency table of models with highest posterior model probability among three models calculated analytically and approximated using the resample-move particle filter with 1000 particles.} \label{tab:cont-1000}
\begin{tabular}{|c|c|c|c|}
  \hline
{\bf 1000 Particles} & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
  \hline
$M_1$ &   2 &   0 &   0 \\
\hline
  $M_2$ &   1 &   7 &   2 \\
  \hline
  $M_3$ &   0 &  2 &   6 \\
   \hline
\end{tabular}
%\end{table}
\quad
%%\begin{table}[h]
%%\centering
%%\caption{Contingency table of models with highest posterior model probability among three models calculated analytically and approximated using the resample-move particle filter with 5000 particles.} \label{tab:cont-5000}
\begin{tabular}{|c|c|c|c|}
  \hline
{\bf 5000 Particles} & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
  \hline
$M_1$ &   2 &   0 &   0 \\
  \hline
  $M_2$ &   1 &   7 &   2 \\
  \hline
  $M_3$ &   0 &   3 &   5 \\
   \hline
\end{tabular}
\end{table}
%\end{minipage}
%\end{table}

We can also evaluate the particle filter approximation of posterior model probabilities by representing the set of probabilities of each model for a particular simulated time series as a point on a triangle, where each corner represents one of the three models. Figure \ref{fig:ternary} displays this for of each simulated data set as well as for particle filter approximations with increasing number of particles. Figure \ref{fig:binary} compares the particle filter approximation of posterior model probabilities with the truth when considering only pairwise comparisons of models $M_1$ and $M_3$ with the true model, $M_2$.

\begin{figure}
\includegraphics[width=1.0\textwidth]{../graphs/cv_pf_ternary-bw}
\caption{Ternary compositional plots of posterior model probabilities of $M_1$ ($\tilde{W} = 0.5$), $M_2$ ($\tilde{W} = 1$), and $M_3$ ($\tilde{W} = 2$) calculated analytically (top left) and approximated using the resample-move particle filter with increasing number of particles. Each point represents one simulated time series and the probability of a model generating a time series of observations is visualized by that point's proximity to that model's corner of the triangle. Points plotted in the same shade of gray correspond to the same simulation (to compare between plot panels).} \label{fig:ternary}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\textwidth]{../graphs/cv_pf_binary}
\caption{Posterior model probabilities calculated analytically (left side of plot panels) and approximated using the resample-move particle filter (right side of plot panels) with increasing number of particles (columns) when comparing $M_2$ against $M_1$ (top row) and $M_2$ against $M_3$ (bottom row). Calculations and particle filter runs were performed on 20 data sets simulated from $M_2$ with $\sigma^2 = 1$.} \label{fig:binary}
\end{figure}

\clearpage

\bibliographystyle{plainnat}
\bibliography{jarad}

\end{document} 