\documentclass{article}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}

\graphicspath{
{/Users/Danny/Documents/"UCSB - Research"/smc/graphs/dlmRW/},
{/Users/Danny/Documents/"UCSB - Research"/smc/graphs/}
}

\begin{document}

\section{Model} \label{sec:model}

We can represent a DLM as a state space model with observation and state equations given by

\begin{align}
y_t &= Fx_t + v_t \label{eqn:obs} \\
x_t &= Gx_{t-1} + w_t \label{eqn:state}
\end{align}

\noindent where $v_t$ and $w_t$ are iid Gaussian random variables with mean 0 and covariance matrices $V$ ($q$ by $q$) and $W$ ($p$ by $p$) , respectively. We also assume $v_t \perp w_{t'}$ for all $t$ and $t'$. $y_t$ ($q$ by $1$) is the observed data at time $t$, and $x_t$ ($p$ by $1$)  is the unknown state at time $t$. We regard $F$ ($q$ by $p$) and $G$ ($p$ by $p$) as known, and consider the special case where $V = \sigma^2\tilde{V}$ and $W = \sigma^2\tilde{W}$, where $\tilde{V}$ and $\tilde{W}$ are known and $\sigma^2$ is an unknown parameter. A fully Bayesian model is specified by defining the prior $p(x_0,\sigma^2)$, which we give as \[\sigma^2 \sim \mbox{IG}(a_0,b_0) \qquad x_0|\sigma^2 \sim \mbox{N}(m_0,\sigma^2\tilde{C}_0)\]
\noindent where $\alpha_0$, $\beta_0$, $m_0$, and $\tilde{C}_0$ are known, $\mbox{IG}(a,b)$ represents the inverse-gamma distribution with shape parameter $a$ and rate parameter $b$, and $N(\mu,\sigma^2)$ represents the normal distribution with mean $\mu$ and variance $\sigma^2$. We could also define the model in terms of the precision, $\phi = 1 / \sigma^2$, and specify the prior $p(x_0,\phi)$ by \[\phi \sim \mbox{G}(\alpha_0,\beta_0) \qquad x_0|\phi \sim \mbox{N}(m_0,\phi^{-1}\tilde{C}_0)\]
\noindent where $G(a,b)$ represents the gamma distribution with shape parameter $a$ and rate parameter $b$.

\section{Estimation} \label{sec:estimation}

Let $y_{1:t} = (y_1,y_2,\ldots,y_t)'$ represent the vector of data observed up until time $t$, $x_{0:t} = (x_0,x_1,x_2,\ldots,x_t)'$ the vector of unknown states up until time $t$, and let $\theta$ denote the vector of any unknown fixed parameters in the model. In general, the posterior distribution $p(x_t,\theta|y_{1:t})$ is not analytically tractable for models of the form given in \eqref{eqn:obs} and \eqref{eqn:state}. However, we can calculate this distribution for the special case described in Section \ref{sec:model}.

We can update the calculation of $p(x_t,\theta|y_{1:t})$ as data arrives sequentially using recursive equations. Denote the prior distribution $p(x_0, \phi)$ defined in Section \ref{sec:model} by $(x_0, \phi) \sim \mbox{NG}(m_0, \tilde{C}_0, \alpha_0, \beta_0)$. Given \[(x_{t-1},\phi)|y_{1:t-1} \sim \mbox{NG}(m_{t-1},\tilde{C}_{t-1}, \alpha_{t-1}, \beta_{t-1})\]
\noindent for $t \ge 1$, the posterior distribution $p(x_t,\phi|y_{1:t})$ is $\mbox{NG}(m_t,\tilde{C}_t,\alpha_t,\beta_t)$ where $m_t$, $\tilde{C}_t$, $\alpha_t$, and $\beta_t$ are calculated through the following equations \citep{petris2009dynamic}:

\begin{align}
a_t &= Gm_{t-1} &\qquad \tilde{R}_t &= G\tilde{C}_{t-1}G' + \tilde{W} \label{eqn:state.pred} \\
f_t &= Fa_t &\qquad \tilde{Q_t} &= F\tilde{R}_tF' + \tilde{V} \label{eqn:obs.pred} \\
m_t &= a_t + \tilde{R}_tF'\tilde{Q}_t^{-1}(y_t-f_t) &\qquad \tilde{C}_t &= \tilde{R}_t - \tilde{R}_tF'\tilde{Q}_t^{-1}F\tilde{R}_t \\
\alpha_t &= \alpha_{t-1} + q / 2 &\qquad \beta_t &= \beta_{t-1} + \frac{1}{2}(y_t-f_t)'\tilde{Q}_t^{-1}(y_t-f_t)
\end{align}

\noindent where $q$ is the dimension of $y_t$.

We can also calculate the marginal posterior distributions $p(x_t|y_{1:t})$ and $p(\phi|y_{1:t})$, given by

\begin{equation}
x_t|y_{1:t} \sim \mbox{T}(m_t,\tilde{C}_t \frac{\beta_t}{\alpha_t},2\alpha_t) \qquad \phi|y_{1:t} \sim \mbox{G}(\alpha_t,\beta_t) \label{eqn:margpost}
\end{equation}

In addition, we can calculate the one-step ahead predictive density $p(y_t|y_{1:t-1})$ using equations \eqref{eqn:state.pred} and \eqref{eqn:obs.pred} since \[y_t|y_{1:t-1} \sim \mbox{T}(f_t,\tilde{Q}_t \frac{\beta_{t-1}}{\alpha_{t-1}},2\alpha_{t-1})\]
\noindent where $\mbox{T}(\mu,\Sigma,d)$ represents the non-standard student-t distribution with location parameter $\mu$, scale parameter $\Sigma$, and $d$ degrees of freedom.

\section{Resample-move algorithm} \label{sec:rm}

As mentioned in Section \ref{sec:estimation}, models of the form described by equations \eqref{eqn:obs} and \eqref{eqn:state} generally do not emit tractable forms of the posterior distributions $p(x_{0:t},\theta|y_{1:t})$ or $p(x_t,\theta|y_{1:t})$. Thus, in many cases we must turn toward methods for approximating these distributions such as particle filtering.

Using particle filtering, we can approximate the posterior distribution $p(x_{0:t},\theta|y_{1:t})$ through a weighted sample approximation, each sample being termed a \emph{particle}. Let $x^{(j)}_{0:t}$ be the vector of unknown states up until time $t$ for particle $j$, $\theta^{(j)}_t$ be the values of the fixed parameters at time $t$ for particle $j$, and $w^{(j)}_t$ be the weight of particle $j$ at time $t$. We require that $\sum_{j=1}^J w^{(j)}_t = 1$, where $J$ is the total number of particles.

We initialize the algorithm by drawing $\left(x^{(j)}_0, \theta^{(j)}_0\right)$ from some prior distribution $p(x_0,\theta_0)$ and setting $w^{(j)}_0 = 1 / J$ for $j = 1,\ldots,J$. Given a particle sample approximation of the posterior $p(x_{0:t-1},\theta|y_{1:t-1})$ - i.e. $\left(x^{(j)}_{0:t-1},\theta^{(j)}_{t-1}\right)$ with associated weights $w^{(j)}_{t-1}$ for $j=1,\ldots,J$ - we move to a particle sample approximation of $p(x_{0:t},\theta|y_{1:t})$ by the following steps:

\begin{enumerate}
\item Augmentation: For each $j = 1,\ldots,J$, draw $\tilde{x}^{(j)}_t$ from some state evolution distribution $p\left(x_t|x^{(j)}_{t-1},\theta^{(j)}_{t-1}\right)$. Add $\tilde{x}^{(j)}_t$ to particle $j$ and denote the new augmented particle by $\left(\tilde{x}^{(j)}_{0:t},\theta^{(j)}_{t-1}\right)$.
\item Update weights: For each $j = 1,\ldots,J$, calculate the \emph{incremental weight} $\alpha^{(j)}_t = p\left(y_t|\tilde{x}^{(j)}_t,\theta^{(j)}_{t-1}\right)$ and then the \emph{unnormalized weight} $\bar{w}^{(j)}_t = w^{(j)}_{t-1}\alpha^{(j)}_t$.
\item Renormalize weights: For each $j = 1,2,\ldots,J$, calculate the \emph{normalized weights}, $\tilde{w}^{(j)}_t$, by setting $\tilde{w}^{(j)}_t = \bar{w}^{(j)}_t / \sum_{l=1}^J \bar{w}^{(l)}_t$.
\item Resample (optional): For each $j$, sample an index $k_j$ from $\{1,2,\ldots,j,\ldots,J\}$ with associated probabilities $\{\tilde{w}^{(1)}_t,\tilde{w}^{(2)}_t,\ldots,\tilde{w}^{(j)}_t,\ldots,\tilde{w}^{(J)}_t\}$.
\item {\tt if( }resampling was performed{\tt )\{} \\
\begin{enumerate}[label=\alph*.]
\item Update weights: Set $w^{(j)}_t = 1 / J$ for all $j$.
\item Move particles: For each $j = 1,2,\ldots,J$, draw a new particle $\left(x^{(j)}_{0:t},\theta^{(j)}_t\right)$ from some transition kernel $q\left(x_{0:t},\theta|\tilde{x}^{(j)}_{0:t},\theta^{(j)}_{t-1}\right)$ with invariant distribution $p(x_{0:t},\theta|y_{1:t})$.
\end{enumerate}
{\tt \} else \{} For each $j = 1,2,\ldots,J$, set $w^{(j)}_t = \tilde{w}^{(j)}_t$ and $\left(x^{(j)}_{0:t},\theta^{(j)}_t\right) = \left(\tilde{x}^{(j)}_{0:t},\theta^{(j)}_{t-1}\right)$.{\tt\}}
\end{enumerate}

The `Resample' step is optional in the sense that if particle weights are not out of balance, it is not necessary to resample the particles because the weighted sample of particles would still provide a good approximation of the posterior. In this case, it would also not be necessary to move the particles, since the problem of degeneracy in the fixed parameters caused by resampling would not be present. By only resampling and moving particles at iterations of the particle filter for which it is necessary, we increase the efficiency of the algorithm. Measures of nonuniformity of particle weights such as effective sample size, coefficient of variation, and entropy can be used to determine when resampling should be performed \citep{Liu:Chen:Wong:reje:1998}. %We use an effective sample size threshold of 80\% of the total number of particles to determine when resample and move particles.

In addition, there are different ways to implement the `Resample' step that may be more efficient under certain conditions. The algorithm as written above implicity uses a \emph{multinomial} resampling scheme, i.e. when we say `sample an index $k_j$ ...'. Additional techniques for resampling exist including \emph{residual}, \emph{stratified}, and \emph{systematic} resampling. \citet{Douc:Capp:Moul:comp:2005} provides an overview of these methods. %We use stratified resampling.

\section{Model Comparison}

Using the algorithm described in Section \ref{sec:rm} we obtain an approximation to $p(\theta_t|y_{1:t})$ through $\sum_{j=1}^J w_{j,t}\delta_{\theta_{j,t}}(x)$, where $\delta_{x_0}(x)$ is the Dirac delta mass function centered at $x_0$. In addition we can use the weighted particle sample to obtain an approximation to the marginal likelihood $p(y_{1:t})$. We first note that given $p(y_{1:t-1})$, $p(y_{1:t})$ can be updated recursively by

\begin{equation}
p(y_{1:t}) = p(y_t|y_{1:t-1})p(y_{1:t-1}) \label{eqn:marglik}
\end{equation}

\noindent Next, we note that an approximation of $p(y_t|y_{1:t-1})$ can be obtained from the weighted particle sample by
\begin{equation}
p(y_t|y_{1:t-1}) \approx \sum_{j=1}^J w^{(j)}_{t-1}\alpha^{(j)}_t \label{eqn:condmarg}
\end{equation}

Having now prescribed a method for approximating $p(y_{1:t})$ sequentially within the resample-move algorithm, we can compare a set of possible models ${M_1,M_2,\ldots,M_p}$ by calculating posterior model probabilities by
\begin{equation}
p(M_i|y_{1:t}) = \frac{p(y_{1:t}|M_i)p(M_i)}{\sum_{i=1}^p p(y_{1:t}|M_i)p(M_i)} \label{eqn:modelcomp}
\end{equation}

\section{Example}

Consider models of the form described in \eqref{eqn:obs} and \eqref{eqn:state}. For each model, we let $V = \sigma^2\tilde{V}$, $W = \sigma^2\tilde{W}$, and known values $F = G = \tilde{V} = 1$. We let $\tilde{W}$ be known at values 0.1, 0.5, 1, 2, and 3 for models $M_1$, $M_2$, $M_3$, $M_4$, and $M_5$, respectively. Given data $y_{1:T}$ that we suspect was generated according to one of these models and a set of prior model probabilities, posterior model probabilities can be calculated exactly using equations \eqref{eqn:marglik} and \eqref{eqn:modelcomp} since the predictive distribution $p(y_t|y_{1:t-1})$ is known in this case to follow a Student-t distribution. We can compare these values with those obtained from running the resample-move particle filter and then using equations \eqref{eqn:condmarg} and \eqref{eqn:modelcomp} to approximate the posterior model probabilities.

A single time series of data, $y_{1:T}$ with $T = 150$, was simulated from $M_2$ with common observation and state equation variance $\sigma^2$ set equal to 1. From this simulated data, we can calculate directly the marginal likelihood of observing this data under $M_3$ (the "true" model). Figure \ref{fig:loglikvW} shows the calculated log marginal likelihood of the data for different models. We can also calculate the marginal posterior distributions $p(x_t|y_{1:t})$, $p(\phi|y_{1:t})$, and one-step ahead predictive distributions $p(y_t|y_{1:t-1})$ for $t = 1,2,\ldots,200$. This allows us to compute 95\% credible intervals for the unknown states and parameter and 95\% one-step ahead prediction intervals for each data point, displayed in Figure \ref{fig:sim}.

\begin{figure}
\includegraphics[width=1.0\textwidth]{cv_loglikvsW}
\caption{Calculated log marginal likelihood of a single simulated time series from $M_3$ for increasing values of $\tilde{W}$, with vertical lines denoting values corresponding to models $M_1$ through $M_5$ (solid line for $M_3$).} \label{fig:loglikvW}
\end{figure}

\begin{figure}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=1.0\textwidth]{cv-test-states-1}
\end{minipage}
\begin{minipage}{0.5\textwidth}
\includegraphics[width=1.0\textwidth]{cv-test-precision-1}
\end{minipage}
\caption{On left, sequential 95\% credible intervals (red lines) for $x_t|y_{1:t}$ and 95\% prediction intervals (blue lines) for $y_t|y_{1:t-1}$ (right) for $t = 1, 2, \ldots, 150$ under $M_3$. On right, sequential 95\% credible intervals (red lines) for $\phi|y_{1:t}$ for $t = 1, 2, \ldots, 150$. Black horizontal lines indicate the true simulated states (left) the true precision (right) used for simulation ($\phi = 1$) . Black points in left picture represent simulated data points. Credible intervals were calculated analytically using the student-t distribution.} \label{fig:sim}
\end{figure}

To assess the variability of the approximation to the log marginal likelihood of the simulated data obtained using the resample-move particle filter, we ran the particle filter 20 times for each of models $M_1$, $M_2$, $M_3$, $M_4$, and $M_5$ on the simulated data from Figure \ref{fig:sim} using 500, 1000, 5000, and 10000 particles. Figure \ref{fig:pf-hist} compares kernel density estimates of the distribution of log marginal likelihoods calculated from these particle filter runs. Figures \ref{fig:pf-1sim-ternary1}, \ref{fig:pf-1sim-ternary2}, and \ref{fig:pf-1sim-ternary3} display compositional plots of posterior model probabilities for each particle filter run compared with the true posterior model probabilities.

\begin{figure}
\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{cv_pf_loglik-1-1-5-10}
\end{minipage}
\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{cv_pf_loglik-1-5-10-20}
\end{minipage}
\begin{minipage}{1.0\linewidth}
\includegraphics[width=1.0\textwidth]{cv_pf_loglik-1-10-20-30}
\end{minipage}
\caption{Three-way comparisons (rows) among different models (columns) of kernel density estimates of log marginal likelihoods approximated using the resample-move particle filter for each of 20 particle filter runs using 500 (red), 1000 (green), 5000 (turquoise), and 10000 (blue) particles. All particle filter runs were performed on a single simulated time series from $M_3$.} \label{fig:pf-hist}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\textwidth]{cv_pf_ternary-1-1-5-10}
\caption{Ternary compositional plots of posterior model probabilities of $M_1$ ($\tilde{W} = 0.1$), $M_2$ ($\tilde{W} = 0.5$), and $M_3$ ($\tilde{W} = 1$) approximated using the resample-move particle filter repeatedly on the same data set (black circles), compared with the true posterior model probabilities for that data set (red circle), for increasing number of particles (different plot panels). The probability of a model generating a time series of observations is visualized by each point's proximity to that model's corner of the triangle.} \label{fig:pf-1sim-ternary1}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\textwidth]{cv_pf_ternary-1-5-10-20}
\caption{Ternary compositional plots of posterior model probabilities of $M_2$ ($\tilde{W} = 0.5$), $M_3$ ($\tilde{W} = 1$), and $M_4$ ($\tilde{W} = 2$) approximated using the resample-move particle filter repeatedly on the same data set (black circles), compared with the true posterior model probabilities for that data set (red circle), for increasing number of particles (different plot panels). The probability of a model generating a time series of observations is visualized by each point's proximity to that model's corner of the triangle.} \label{fig:pf-1sim-ternary2}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\textwidth]{cv_pf_ternary-1-10-20-30}
\caption{Ternary compositional plots of posterior model probabilities of $M_3$ ($\tilde{W} = 1$), $M_4$ ($\tilde{W} = 2$), and $M_5$ ($\tilde{W} = 3$) approximated using the resample-move particle filter repeatedly on the same data set (black circles), compared with the true posterior model probabilities for that data set (red circle), for increasing number of particles (different plot panels). The probability of a model generating a time series of observations is visualized by each point's proximity to that model's corner of the triangle.} \label{fig:pf-1sim-ternary3}
\end{figure}

Next, we simulated 19 more data sets from $M_3$ with $\sigma^2 = 1$ (making 20 total simulated data sets). For each data set, the log marginal likelihoods and posterior model probabilities under models $M_2$ through $M_4$ were calculated analytically. In addition, the resample-move particle filter was run with 100, 500, 1000, and 5000 particles under each of the three models for each simulated data set. Thus, for each time series of data, we can compare which model has the highest true posterior probability with the one that has highest posterior probability as approximated by the resample-move particle filter. Table \ref{tab:cont} displays these results for increasing number of particles used in the resample-move algorithm. If the particle filter approximation is in line with the true posterior, then all off-diagonal cells in the tables should be 0.

\begin{table}[h]
\centering
\caption{Contingency table of models with highest posterior model probability calculated analytically (rows) and approximated using the resample-move particle filter (columns) with increasing number of particles (tables).} \label{tab:cont}
\begin{tabular}{|c|c|c|c|}
  \hline
{\bf 500 Particles} & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
  \hline
$M_1$ &    3 &   2 &   0 \\
\hline
$M_2$ &   0 &   4 &   2 \\
\hline
$M_3$ &   0 &   1 &   8 \\
   \hline
\end{tabular}
\quad
\begin{tabular}{|c|c|c|c|}
  \hline
{\bf 1000 Particles} & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
  \hline
$M_1$ &     4 &   1 &   0 \\
\hline
$M_2$ &    0 &   4 &   2 \\
\hline
$M_3$ &   0 &   3 &   6 \\
   \hline
\end{tabular}
%\end{table}
\quad
%\begin{table}[h]
%\centering
%\caption{Contingency table of models with highest posterior model probability among three models calculated analytically and approximated using the resample-move particle filter with 1000 particles.} \label{tab:cont-1000}
\begin{tabular}{|c|c|c|c|}
  \hline
{\bf 5000 Particles} & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
  \hline
$M_1$ &    3 &   2 &   0 \\
\hline
  $M_2$ & 0 &   5 &   1 \\
  \hline
  $M_3$ &    0 &   1 &   8 \\
   \hline
\end{tabular}
%\end{table}
\quad
%%\begin{table}[h]
%%\centering
%%\caption{Contingency table of models with highest posterior model probability among three models calculated analytically and approximated using the resample-move particle filter with 5000 particles.} \label{tab:cont-5000}
\begin{tabular}{|c|c|c|c|}
  \hline
{\bf 10000 Particles} & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
  \hline
$M_1$ &  4 &   1 &   0 \\
  \hline
  $M_2$ &   0 &   5 &   1 \\
  \hline
  $M_3$ &    0 &   1 &   8 \\
   \hline
\end{tabular}
\end{table}
%\end{minipage}
%\end{table}

We can also evaluate the particle filter approximation of posterior model probabilities by representing the set of probabilities of each model for a particular simulated time series as a point on a triangle, where each corner represents one of the three models. Figure \ref{fig:ternary} displays this for of each simulated data set as well as for particle filter approximations with increasing number of particles. Figure \ref{fig:binary} compares the particle filter approximation of posterior model probabilities with the truth when considering only pairwise comparisons of models $M_2$ and $M_4$ with the true model, $M_3$.

\begin{figure}
\includegraphics[width=1.0\textwidth]{cv_pf_ternary-bw}
\caption{Ternary compositional plots of posterior model probabilities of $M_2$ ($\tilde{W} = 0.5$), $M_3$ ($\tilde{W} = 1$), and $M_4$ ($\tilde{W} = 2$) calculated analytically (top left) and approximated using the resample-move particle filter with increasing number of particles. Each point represents one simulated time series and the probability of a model generating a time series of observations is visualized by that point's proximity to that model's corner of the triangle. Points plotted in the same shade of gray correspond to the same simulation (to compare between plot panels).} \label{fig:ternary}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\textwidth]{cv_pf_binary}
\caption{Posterior model probabilities calculated analytically (left side of plot panels) and approximated using the resample-move particle filter (right side of plot panels) with increasing number of particles (columns) when comparing $M_3$ against $M_2$ (top row) and $M_3$ against $M_4$ (bottom row). Calculations and particle filter runs were performed on 20 data sets simulated from $M_3$ with $\sigma^2 = 1$.} \label{fig:binary}
\end{figure}

\clearpage

\section{Example - fMRI}

Next, we consider the possibility of model comparison in a situation where we are not able to obtain an explicit form of the true marginal likelihood. Specifically, we can consider data $y_t$ that represents a time series from one voxel of an fMRI experiment. We propose two possible models for the data:
\begin{align}
y_t &= \beta_0 + (\beta_1 + x_t)\mbox{conv}_t + v_t \label{eqn:M101} \\
y_t &= \beta_0 + \beta_1\mbox{conv}_t + x_t + v_t \label{eqn:M011}
\end{align}
\noindent where $x_t$ follows an AR(1) process with autocorrelation parameter $\phi$ and Gaussian white noise variance $\sigma^2_s$ (i.e. $x_t = \phi x_{t-1} + w_t$ with $w_t \stackrel{\mbox{iid}}{\sim} \mbox{N}(0,\sigma^2_s)$), $v_t \stackrel{\mbox{iid}}{\sim} \mbox{N}(0,\sigma^2_m)$, $\mbox{conv}_t$ is the expected BOLD response at time $t$ (the convolution of the haemodynamic response function with the experimental stimulus pattern), and $\beta_0$ and $\beta_1$ are fixed parameters representing the baseline level of BOLD activity and the slope coefficient for the linear relationship between the observed and expected BOLD responses, respectively. The hypothesis test $H_0: \beta_1 = 0$ versus $H_1: \beta_1 > 0$ is usually of interest for determining whether the voxel is responsive the experimental stimulus. Equation \eqref{eqn:M101} has a dynamic slope while equation \eqref{eqn:M011} has a constant slope with the autocorrelation pushed into the error term. We refer to the model represented by equation \eqref{eqn:M101} as $M_{101}$ and the model in equation \eqref{eqn:M011} as $M_{011}$.

Both of these models can be represented as a DLM of the form
\begin{align}
y_t = U_t\beta + F_tx_t + v_t \label{eqn:obs} \\
x_t = Gx_{t-1} + w_t \label{eqn:state}
\end{align}
\noindent where $v_t \stackrel{\mbox{iid}}{\sim} \mbox{N}(0,\sigma^2_m)$ independent of $w_t \stackrel{\mbox{iid}}{\sim} \mbox{N}(0,\sigma^2_s)$, $U_t = (1, \mbox{conv}_t)$, $\beta = (\beta_0,\beta_1)'$, and $G = \phi$. For $M_{101}$, we let $F_t = \mbox{conv}_t$ and for $M_{011}$ we let $F_t = 1$ for all $t$.

Eight time series of length $T = 250$ were simulated according to both $M_{101}$ and $M_{011}$ with true parameter values set to \[\beta_0 = 900 \quad \beta_1 = 5 \quad \phi = 0.8 \quad \sigma^2_s = 1 \quad \sigma^2_m = 1\] and the resample move particle filter with 100 particles was run 3 times under each model for each simulated time series. Thus, for each simulation, 3 estimates of the marginal likelihood of each model given the data could be calculated.

To offer a fair comparison between the two models, an additional time series of length $T = 100$ was simulated prior to each resample-move particle filter run using the same true values of the model parameters, and the particle filter was run on the 100-length time series in order construct priors for the subsequent particle filter run. Priors on the unknown parameters for the particle filter run on the 100-length time series were taken to be \[ \beta_0 \sim \mbox{N}(b_0,B_0) \quad \beta_1 \sim \mbox{N}(b_1,B_1) \quad \phi \sim \mbox{N}_{(-1,1)}(\phi_0,\Phi) \quad \sigma^2_s \sim \mbox{IG}(a_{s_0},b_{s_0}) \quad \sigma^2_m \sim \mbox{IG}(a_{m_0},b_{m_0}) \] with $b_0 = 900$, $b_1 = B_0 = B_1 = 5$, $\phi_0 = 0.8$, $\Phi_0 = 0.2$, $a_{s_0} = b_{s_0} = a_{m_0} = b_{m_0} = 1$. The hyperparamters for the priors for the subsequent particle filter runs were then set such that the mean and variance for each prior distribution was matched with the sample mean and sample variance of the ``prior'' posterior sample at $t = 100$. For particle filter runs on both the initial ($T = 100$) and original ($T = 250$) simulated time series, the prior on the initial state $x_0$ was $x_0|\phi,\sigma^2_s \sim \mbox{N}(0,\frac{\sigma^2_s}{1-\phi^2})$, i.e. the stationary distribution of an AR(1) process.

The upper right plots of Figures \ref{fig:fmri-lik-n-M101} and \ref{fig:fmri-lik-n-M011} display the results of the log marginal likelihood calulations for these runs. These results indicate that it is easier to identify $M_{101}$ for these specific parameter values. Eight simulated data sets were then generated from both models with true values of $\phi$ and $\sigma^2$ changed, and these results are displayed in the remaining plot panels of the figures. These results suggest that it becomes easier to identify $M_{101}$ for larger values of $\phi$ and $\sigma^2_s$, and easier to identify $M_{011}$ for smaller values of $\phi$ and $\sigma^2_s$.

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_rm_lik-20-M101-2-8-3-100-TRUE}
\caption{Estimated log marginal likelihood of $M_{101}$ (red) and $M_{011}$ (blue) from 3 different runs of the resample move particle filter with 100 particles on each simulated data set from model $M_{101}$ for increasing true $\phi$ and $\sigma^2_s$.} \label{fig:fmri-lik-n-M101}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_rm_lik-20-M011-2-8-3-100-TRUE}
\caption{Estimated log marginal likelihood of $M_{101}$ (red) and $M_{011}$ (blue) from 3 different runs of the resample move particle filter with 100 particles on each simulated data set from model $M_{011}$ for increasing true $\phi$ and $\sigma^2_s$.} \label{fig:fmri-lik-n-M011}
\end{figure}

\clearpage

For the simulations with $\phi$ and $\sigma^2_s$ set to 0.8 and 1, respectively, additional resample move particle filters were run with prior distributions set such that the prior standard deviations were equal to 3, 5, and 7 times the sample standard devation of the posterior sample generated at $t = 100$ from the initial particle filter run on the $T = 100$ length data set. This was meant to illustrate the point that, even for data simulated from $M_{101}$, $M_{011}$ can yield a higher log marginal likelihood if the prior used for $M_{101}$ is made more diffuse. Figure \ref{fig:fmri-lik-sd-M101} shows log marginal likelihood estimates for increasing prior standard deviation for $M_{101}$ on data simulated from $M_{101}$, and Figure \ref{fig:fmri-lik-sd-M011} shows analogous results for $M_{011}$.

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_rm_lik-20-M101-2-8-3-100-FALSE}
\caption{Estimated log marginal likelihood of $M_{101}$ (red) and $M_{011}$ (blue) from 3 different runs of the resample move particle filter with 100 particles on each simulated data set from model $M_{101}$ for increasing prior standard devation on $M_{101}$.} \label{fig:fmri-lik-sd-M101}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_rm_lik-20-M011-2-8-3-100-FALSE}
\caption{Estimated log marginal likelihood of $M_{101}$ (red) and $M_{011}$ (blue) from 3 different runs of the resample move particle filter with 100 particles on each simulated data set from model $M_{011}$ for increasing prior standard devation on $M_{011}$.} \label{fig:fmri-lik-sd-M011}
\end{figure}

\clearpage

\bibliographystyle{plainnat}
\bibliography{jarad}

\end{document} 