\documentclass{article}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}

\begin{document}

Here we describe estimation of the states and unknown parameters of a state-space model of an epidemic outbreak using MCMC.

\section{Model \label{sec:model}}

Let $x_t = (s_t,i_t)'$ denote the state of the epidemic at time $t$. We consider a compartmental model of disease transmission that is governed by three parameters:

\begin{itemize}
\item $\beta$, the contact rate of spread of disease,
\item $\gamma$, the recovery time from infection, and
\item $\nu$, the mixing intensity of the population.
\end{itemize}

\noindent $\beta$, $\gamma$, and $\nu$ are each restricted to be nonnegative. Define $\theta = (\beta,\gamma,\nu)'$ to be the vector of unknown parameters in our model, and let $P$ be the size of the population. Then, we describe the evolution of the epidemic from time $t$ to $t + 1$ by
\begin{equation}
x_{t+1}\left|x_t,\theta\right. \sim N_\Omega\left(x_{t+1};f(x_t,\theta),Q(\theta)\right) \label{eqn:state}
\end{equation}
\noindent where
\[
f(x_t,\theta) = \left(
\begin{array}{c}
s_t - \beta i_ts^{\nu}_t \phantom{- \gamma i_t}\,\, \\
i_t +  \beta i_ts^\nu_t - \gamma i_t
\end{array}
\right)
\qquad
Q(\theta) = \frac{\beta}{P^2} \left(
\begin{array}{ccccc}
1 & -1 \\
-1 & 1 + \gamma/\beta
\end{array}
\right)
\]

\noindent and $\Omega = \{(s_t,i_t): s_t \ge 0, i_t \ge 0, s_t + i_t \le 1\}$. $N_{\Omega}(.; \mu,\Sigma)$ represents the pdf of the truncated normal distribution onto the set $\Omega$ with mean and covariance matrix of its corresponding untruncated normal distribution given by $\mu$ and $\Sigma$, respectively.

We consider observed data that are positive real numbers related to counts of emergency room visits, prescription sales, or calls to a hotline, for example, and we can observe data from these different streams/sources asynchronously in time. That is, at any time $t$, we can observe data from any subset of the streams (or possibly none of them). Let $y_{l,t}$ represent data coming from stream $l$ at time $t$, where $l = 1,2,\ldots,L$ and $t = 1,2,\ldots,T$. We model the log of the observations (so that $y_{l,t}$ is restricted to be positive) by
\begin{equation}
\log y_{l,t} \sim N\left(\log y_{l,t};b_li_t^{\varsigma_l} + \eta_l,\sigma_l^2\right) \label{eqn:obs}
\end{equation}
where $b_l$, $\varsigma_l$, and $\sigma_l$ are nonnegative constants and $\eta_l$ is a real number that determines the baseline level of incoming syndromic data from stream $l$. We assume $b_l$, $\varsigma_l$, $\sigma_l$, and $\eta_l$ are known for all $l$.

\section{Bayesian estimation}

We specify the following priors on the initial state $x_0$ and unknown parameters $\theta$:

\begin{align*}
p(x_0) &= p(i_0,s_0) = N(i_0;0.002,0.005^2)\delta_{1 - i_0}(s_0) \\
p(\theta) &= p(\beta)p(\gamma)p(\nu) \\
p(\beta) &= LN(\beta;-1.3296,0.3248^2) \\
p(\gamma) &= LN(\gamma;-2.1764,0.1183^2) \\
p(\nu) &= LN(\nu;0.1055,0.0800^2)
\end{align*}

We can now derive full conditional distributions of the states and unknown parameters using Bayes' rule. We use the fact that the joint density of the observations, states, and unknown parameters can be calculated by

\begin{equation}
p(y_{1:T},x_{0:T},\theta) = \prod_{t = 1}^T \left\{p(y_t|x_t,\theta)p(x_t|x_{t-1},\theta)\right\}p(x_0,\theta) \label{eqn:joint}
\end{equation}

\noindent where $y_t = (y_{1,t},y_{2,t},\ldots,y_{L,t})'$ and

\begin{align*}
p(y_t|x_t) &= \prod_{l=1}^L \left(\emph{LN}(y_{l,t};b_li_t^{\varsigma_l},\sigma_l^2)\right) \\
p(x_t|x_{t-1},\theta) &= p(s_t,i_t|s_{t-1},i_{t-1},\theta) = p(i_t|s_t,s_{t-1},i_{t-1},\theta)p(s_t|s_{t-1},i_{t-1},\theta) \\
&= N_{[0,1-s_t]}(i_t;i_{t-1}(1 - \gamma) + s_{t-1} - s_t, \gamma / P^2)N_{[0,1]}(s_t; s_{t-1} - \beta i_{t-1}s^{\nu}_{t-1}, \beta / P^2) \\
p(x_0,\theta) &= p(x_0)p(\beta)p(\gamma)p(\nu)
\end{align*}

\section{Full conditionals} \label{sec:fullcond}
We can now express the full conditional distributions of the states $x_t$ for $t = 0, 1, \ldots, T$ and unknown parameters $\beta$, $\gamma$, and $\nu$ up to a proportionality constant by

\begin{align*}
p(x_0|\hdots) &\propto p(x_1|x_0,\theta)p(x_0) \\
p(x_t|\hdots) &\propto p(y_t|x_t)p(x_{t+1}|x_t,\theta)p(x_t|x_{t-1},\theta) \mbox{, for } t = 1,\ldots,T-1 \\
p(x_T|\hdots) &\propto p(y_T|x_T)p(x_T|x_{T-1},\theta) \\
p(\beta|\hdots) &\propto \prod_{t=1}^T \{p(x_t|x_{t-1},\theta)\}p(\beta) \\
p(\gamma|\hdots) &\propto \prod_{t=1}^T \{p(x_t|x_{t-1},\theta)\}p(\gamma) \\
p(\nu|\hdots) &\propto \prod_{t=1}^T \{p(x_t|x_{t-1},\theta)\}p(\nu)
\end{align*}

\section{Gibbs Sampler} \label{sec:gibbs}

We use Gibbs sampling to obtain draws from the joint posterior distribution $p(x_{0:T},\theta|y_{1:T})$. The Gibbs sampling algorithm proceeds in the following manner:

\begin{enumerate}
\item Start with initial draws $x^{(0)}_{0:T} = (x^{(0)}_0,x^{(0)}_1,\ldots,x^{(0)}_T)'$, $\beta^{(0)}$, $\gamma^{(0)}$, and $\nu^{(0)}$. Set $j = 1$.
\item \label{step:gibbs} For $k = 1, 2, \ldots, T$, draw $x^{(j)}_k$ from the full conditional \[p(x_k|x^{(j)}_0,x^{(j)}_1,\ldots,x^{(j)}_{k-1},x^{(j-1)}_k,x^{(j-1)}_{k+1},\ldots,x^{(j-1)}_{T-1},x^{(j-1)}_T,\beta^{(j-1)},\gamma^{(j-1)},\nu^{(j-1)})\].
\item Draw $\beta^{(j)}$ from the full conditional $p(\beta|x^{(j)}_{0:T},\beta^{(j-1)},\gamma^{(j-1)},\nu^{(j-1)},y_{1:T})$
\item Draw $\gamma^{(j)}$ from the full conditional $p(\gamma|x^{(j)}_{0:T},\beta^{(j)},\gamma^{(j-1)},\nu^{(j-1)},y_{1:T})$
\item Draw $\nu^{(j)}$ from the full conditional $p(\nu|x^{(j)}_{0:T},\beta^{(j)},\gamma^{(j)},\nu^{(j-1)},y_{1:T})$
\item Set $j = j + 1$ and go back to step \ref{step:gibbs}
\end{enumerate}

\section{Metropolis-Hastings}
To obtain the draws from the full conditional distributions at each step of the Gibbs sampler, we use a random-walk Metropolis Hastings algorithm. The algorithm proceeds as follows:

\begin{enumerate}
\item Let $f(x) = f(x|\hdots)$ represent the full conditional distribution of interest expressed up to a proportionality constant (i.e. as in Section \ref{sec:fullcond}) and let $g(x|y)$ represent a symmetric proposal distribution, i.e. $g(x|y) = g(y|x)$.
\item Given $x^{(i-1)}$, sample $x^*$ from $g(x|x^{(i-1)})$.
\item Calculate the log of the Metropolis ratio, $R$, according to
\[\log R = \log f(x^*) - \log f(x^{(i-1)})\]
\item Sample $u$ from the continuous uniform distribution on $[0,1]$. If $\log u < \log R$, set $x^{(i)} = x^*$. Otherwise, set $x^{(i)} = x^{(i-1)}$.
\end{enumerate}

\noindent For all steps of the Gibbs sampler described in Section \ref{sec:gibbs}, we use a Gaussian random walk proposal, i.e. $g(x|y) = N(x;y,\tau^2)$, where $\tau$ is known and adjusted during the burn-in period of the MCMC according to the acceptance rate (i.e., if $x^*$ is accepted, $\tau$ is multiplied by 1.1, and if $x^*$ is rejected, $\tau$ is divided by 1.1.

When sampling an individual state vector $x_k$, $k \in \{1,\ldots,T\}$, $i_k$ and $s_k$ are sampled from independent Gaussian random-walk proposals, but the proposed $x^*_k = (s^*_k,i^*_k)'$ is accepted or rejected jointly. For $k = 0$, $i^*_0$ is sampled from a Gaussian random-walk proposal, and $s^*_0$ is set to $1 - i^*_0$.

\end{document} 