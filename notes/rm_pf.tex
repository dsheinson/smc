\documentclass{article}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}

\begin{document}

\section{Model} \label{sec:model}

We can represent a DLM as a state space model with observation and state equations given by

\begin{align}
y_t &= Fx_t + v_t \label{eqn:obs} \\
x_t &= Gx_{t-1} + w_t \label{eqn:state}
\end{align}

\noindent where $v_t$ and $w_t$ are iid Gaussian random variables with mean 0 and covariance matrices $V$ and $W$, respectively. We also assume $v_t \perp w_{t'}$ for all $t$ and $t'$. $y_t$ is the observed data at time $t$, and $x_t$ is the unknown state at time $t$. We regard $F$ and $G$ as known, and consider the special case where $V = \sigma^2\tilde{V}$ and $W = \sigma^2\tilde{W}$, where $\tilde{V}$ and $\tilde{W}$ are known and $\sigma^2$ is an unknown parameter. A fully Bayesian model is specified by defining the prior $p(x_0,\sigma^2)$, which we give as \[\sigma^2 \sim \mbox{IG}(a_0,b_0) \qquad x_0|\sigma^2 \sim \mbox{N}(m_0,\sigma^2\tilde{C}_0)\]
\noindent where $\alpha_0$, $\beta_0$, $m_0$, and $\tilde{C}_0$ are known, $\mbox{IG}(a,b)$ represents the inverse-gamma distribution with shape parameter $a$ and rate parameter $b$, and $N(\mu,\sigma^2)$ represents the normal distribution with mean $\mu$ and variance $\sigma^2$. We could also define the model in terms of the precision, $\phi = 1 / \sigma^2$, and specify the prior $p(x_0,\phi)$ by \[\phi \sim \mbox{G}(\alpha_0,\beta_0) \qquad x_0|\phi \sim \mbox{N}(m_0,\phi^{-1}\tilde{C}_0)\]
\noindent where $G(a,b)$ represents the gamma distribution with shape parameter $a$ and rate parameter $b$.

\section{Estimation} \label{sec:estimation}

Let $y_{1:t} = (y_1,y_2,\ldots,y_t)'$ represent the vector of data observed up until time $t$, $x_{1:t} = (x_1,x_2,\ldots,x_t)'$ the vector of unknown states up until time $t$, and let $\psi$ denote the vector of any unknown fixed parameters in the model. In general, the posterior distribution $p(x_{1:t},\psi|y_{1:t})$ is not analytically tractable for models of the form given in \eqref{eqn:obs} and \eqref{eqn:state}. However, we can calculate this distribution for the special case described in Section \ref{sec:model}.

We can update the calculation of $p(x_{1:t},\psi|y_{1:t})$ as data arrives sequentially using recursive equations. Denote the prior distribution $p(x_0, \phi)$ defined in Section \ref{sec:model} by $(x_0, \phi) \sim \mbox{NG}(m_0, \tilde{C}_0, \alpha_0, \beta_0)$. Given \[(x_{t-1},\phi)|y_{1:t-1} \sim \mbox{NG}(m_{t-1},\tilde{C}_{t-1}, \alpha_{t-1}, \beta_{t-1})\]
\noindent for $t \ge 1$, the posterior distribution $p(x_t,\phi|y_{1:t})$ is $\mbox{NG}(m_t,\tilde{C}_t,\alpha_t,\beta_t)$ where $m_t$, $\tilde{C}_t$, $\alpha_t$, and $\beta_t$ are calculated through the following equations \citep{petris2009dynamic}:

\begin{align}
a_t &= Gm_{t-1} &\qquad \tilde{R}_t &= G\tilde{C}_{t-1}G + W \label{eqn:state.pred} \\
f_t &= Fa_t &\qquad \tilde{Q_t} &= F\tilde{R}_tF + V \label{eqn:obs.pred} \\
m_t &= a_t + \tilde{R}_tF\tilde{Q}_t^{-1}(y_t-f_t) &\qquad \tilde{C}_t &= \tilde{R}_t - \tilde{R}_tF'\tilde{Q}_t^{-1}F\tilde{R}_t' \\
\alpha_t &= \alpha_{t-1} + q / 2 &\qquad \beta_t &= \beta_{t-1} + (y_t-f_t)'\tilde{Q}_t^{-1}(y_t-f_t)
\end{align}

\noindent where $q$ is the dimension of $y_t$.

We can also calculate the marginal posterior distributions $p(x_t|y_{1:t})$ and $p(\phi|y_{1:t})$, given by

\begin{equation}
x_t|y_{1:t} \sim \mbox{T}(m_t,\tilde{C}_t \frac{\beta_t}{\alpha_t},2\alpha_t) \qquad \phi|y_{1:t} \sim \mbox{G}(\alpha_t,\beta_t) \label{eqn:margpost}
\end{equation}

In addition, we can calculate the one-step ahead predictive density $p(y_t|y_{1:t-1})$ using equations \eqref{eqn:state.pred} and \eqref{eqn:obs.pred} since \[y_t|y_{1:t-1} \sim \mbox{T}(f_t,\tilde{Q}_t \frac{\beta_{t-1}}{\alpha_{t-1}},2\alpha_{t-1})\]
\noindent where $\mbox{T}(\mu,\Sigma,d)$ represents the non-standard student-t distribution with location parameter $\mu$, scale parameter $\Sigma$, and $d$ degrees of freedom.

\section{Resample-move algorithm} \label{sec:rm}

As mentioned in Section \ref{sec:estimation}, models of the form described by equations \eqref{eqn:obs} and \eqref{eqn:state} generally do not emit tractable forms of the posterior distribution $p(x_{1:t},\psi|y_{1:t})$. Thus, in many cases we must turn toward methods for approximating this distribution such as particle filtering. One specific type of particle filter known as the \emph{resample-move} algorithm was developed by \citet{Gilk:Berz:foll:2001, Berz:gilk:2001} and is summarized here.

Let $y_{1:t} = (y_1,y_2,\ldots,y_t)'$ represent the vector of data observed up until time $t$ and let $\theta_t$ be a vector of unknown states and/or parameters at time $t$. We allow $\theta_t$ to change with time, and we consider in particular the case where the dimension of $\theta_t$ changes with time, e.g. if $\theta_t$ contains an evolving trajectory of states. Let $\theta^{+}_t$ be the (possibly empty) set of unknown states entering the model at time $t+1$. Then, we can define $\theta_{t+1} = (\theta_t,\theta^{+}_t)'$. For the DLM described in Section \ref{sec:model}, we might wish to keep track of the entire trajectory of unknown states up to time $t$, in which case $\theta_t = (x_0,x_1,\ldots,x_t,\sigma^2)'$ and $\theta^{+}_t = x_{t+1}$.

We use particle filtering to approximate the posterior distribution $p(\theta_t|y_{1:t})$ through a weighted sample approximation, each sample being termed a \emph{particle}. Let $\theta_{j,t}$ be the vector of unknown states and/or parameters for particle $j$ at time $t$ and $w_{j,t}$ be the weight of particle $j$ at time $t$. We require that $\sum_{j=1}^J w_{j,t} = 1$, where $J$ is the total number of particles.

We initialize the algorithm by drawing $\theta_{j,0}$ from some prior distribution $p(\theta_0)$ and setting $w_{j,0} = 1 / J$ for $j = 1,\ldots,J$. Given a particle sample approximation of the posterior $p(\theta_{t-1}|y_{1:t-1})$ - i.e. $\left(\theta_{j,t-1}, w_{j,t-1}\right)$ for $j=1,\ldots,J$ - we move to a particle sample approximation of $p(\theta_t|y_{1:t})$ by the following steps:

\begin{enumerate}
\item Augmentation: For each $j = 1,\ldots,J$, draw $\theta^{+}_{j,t-1}$ from some state evolution distribution $p(\theta^{+}_{t-1}|\theta_{j,t-1})$ and define a new particle $\theta^{*}_{j,t} = (\theta_{j,t-1},\theta^{+}_{j,t-1})'$.
\item Update weights: For each $j = 1,\ldots,J$, calculate the incremental weight $\alpha_{j,t} = p(y_t|\theta^{*}_{j,t-1})$ and then the  unnormalized weight $w^{**}_{j,t} = w_{j,t-1}\alpha_{j,t}$.
\item Renormalize the $w^{**}_{j,t}$'s by setting $w^{*}_{j,t} = w^{**}_{j,t} / \sum_{l=1}^J w^{**}_{l,t}$.
\item Resample (optional): For each $j$, sample an index $k_j$ from $\{1,2,\ldots,J\}$ with associated probabilities $\{w^{*}_{1,t},w^{*}_{2,t},\ldots,w^{*}_{j,t},\ldots,w^{*}_{J,t}\}$.
\item {\tt if(}resampling was performed{\tt )\{} \\
\begin{enumerate}[label=\alph*.]
\item Update weights: Set $w_{j,t} = 1 / J$ for all $j$.
\item Move particles: For each $j = 1,2,\ldots,J$, draw $\theta_{j,t}$ from some transition kernel $q(\theta_t|\theta^{*}_{k_j,t})$ with invariant distribution $p(\theta_t|y_{1:t})$.
\end{enumerate}
{\tt \} else \{} Set $w_{j,t} = w^{*}_{j,t}$ and $\theta_{j,t} = \theta^{*}_{j,t}$ for all $j$.\}
\end{enumerate}

The `Resample' step is optional in the sense that if particle weights are not out of balance, it is not necessary to resample the particles because the weighted sample of particles would still provide a good approximation of the posterior. In this case, it would also not be necessary to move the particles, since the problem of degeneracy in the fixed parameters caused by resampling would not be present. By only resampling and moving particles at iterations of the particle filter for which it is necessary, we increase the efficiency of the algorithm. Measures of nonuniformity of particle weights such as effective sample size, coefficient of variation, and entropy can be used to determine when resampling should be performed \citep{Liu:Chen:Wong:reje:1998}. %We use an effective sample size threshold of 80\% of the total number of particles to determine when resample and move particles.

In addition, there are different ways to implement the `Resample' step that may be more efficient under certain conditions. The algorithm as written above implicity uses a \emph{multinomial} resampling scheme, i.e. when we say `sample an index $k_j$ ...'. Additional techniques for resampling exist including \emph{residual}, \emph{stratified}, and \emph{systematic} resampling. \citet{Douc:Capp:Moul:comp:2005} provides an overview of these methods. %We use stratified resampling.

\section{Model Comparison}

Using the algorithm described in Section \ref{sec:rm} we obtain an approximation to $p(\theta_t|y_{1:t})$ through $\sum_{j=1}^J w_{j,t}\delta_{\theta_{j,t}}(x)$, where $\delta_{x_0}(x)$ is the Dirac delta mass function centered at $x_0$. In addition we can use the weighted particle sample to obtain an approximation to the marginal likelihood $p(y_{1:t})$. We first note that given $p(y_{1:t-1})$, $p(y_{1:t})$ can be updated recursively by

\begin{equation}
p(y_{1:t}) = p(y_t|y_{1:t-1})p(y_{1:t-1}) \label{eqn:marglik}
\end{equation}

\noindent Next, we note that an approximation of $p(y_t|y_{1:t-1})$ can be obtained from the weighted particle sample by
\begin{equation}
p(y_t|y_{1:t-1}) \approx \sum_{j=1}^J w_{j,t-1}\alpha_{j,t} \label{eqn:condmarg}
\end{equation}

Having now prescribed a method for approximating $p(y_{1:t})$ sequentially within the resample-move algorithm, we can compare a set of possible models ${M_1,M_2,\ldots,M_p}$ by calculating posterior model probabilities by
\begin{equation}
p(M_i|y_{1:t}) = \frac{p(y_{1:t}|M_i)p(M_i)}{\sum_{i=1}^p p(y_{1:t}|M_i)p(M_i)} \label{eqn:modelcomp}
\end{equation}

\section{Example}

Consider three models of the form described in \eqref{eqn:obs} and \eqref{eqn:state}. For each model, we let $V = \sigma^2\tilde{V}$, $W = \sigma^2\tilde{W}$, and known values $F = G = \tilde{V} = 1$. We let $\tilde{W}$ be known at values 0.5, 1, and 2 for models $M_1$, $M_2$, and $M_3$, respectively. Given data $y_{1:T}$ that we suspect was generated according to one of these models and a set of prior model probabilities, posterior model probabilities can be calculated exactly using equations \eqref{eqn:marglik} and \eqref{eqn:modelcomp} since the predictive distribution $p(y_t|y_{1:t-1})$ is known in this case to follow a Student-t distribution. We can compare these values with those obtained from running the resample-move particle filter and then using equations \eqref{eqn:condmarg} and \eqref{eqn:modelcomp} to approximate the posterior model probabilities.

A single time series of data, $y_{1:T}$ with $T = 200$, was simulated from $M_2$ with common observation and state equation variance $\sigma^2$ set equal to 1. From this simulated data, we can calculate directly the marginal likelihood of observing this data under $M_2$ ($\log p(y_{1:200}) = -364.3271$). We can also calculate the marginal posterior distributions $p(x_t|y_{1:t})$ and one-step ahead predictive distributions $p(y_t|y_{1:t-1})$ for $t = 1,2,\ldots,200$. This allows us to compute 95\% credible intervals for the unknown states and parameter and 95\% one-step ahead prediction intervals for each data point, displayed in Figures \ref{fig:sim-states} and \ref{fig:sim-precision}.

\begin{figure}
\includegraphics[width=1.0\textwidth]{../graphs/10-23-13/mc_1sim_test-states}
\caption{95\% credible intervals (left, red lines) of $x_t|y_{1:t}$ and 95\% prediction intervals (right, blue lines) for $y_t|y_{1:t-1}$ calculated explicitly using the Student's-t distribution for $t = 1,2,\ldots,200$. Black lines indicated the true simulated states (left) and observations (right).} \label{fig:sim-states}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\textwidth]{../graphs/10-23-13/mc_1sim_test-precision}
\caption{95\% credible intervals (red lines) for $\phi|y_{1:t}$ for $t = 1, 2, \ldots, 200$. Black horizontal line indicates the true value used for simulation ($\phi = 1$).} \label{fig:sim-precision}
\end{figure}

To assess the variability of the approximation to the log marginal likelihood of the simulated data obtained using the resample-move particle filter, we ran the particle filter 20 times each using 100, 500, and 1000 particles. Figure \ref{fig:pf-hist} displays histograms of the log marginal likelihoods calculated from these particle filter runs.

\begin{figure}
\includegraphics[width=1.0\textwidth]{../graphs/10-23-13/mc_1sim_test-hist}
\caption{Histograms of log marginal likelihoods of a single time series simulated from $M_2$, approximated assuming $M_2$ true using the resample-move particle filter, for each of 20 particle filter runs using 100 (white), 500 (gray), and 1000 (black) particles.} \label{fig:pf-hist}
\end{figure}

Next, we simulated 20 more data sets from $M_2$ with $\sigma^2 = 1$. For each data set, posterior probabilities among models $M_1$, $M_2$, and $M_3$ were calculated using the analytic calculation of the marginal likelihood as well as the resample-move particle filter with 100, 500, and 1000 particles.

\begin{table}
\begin{center}
\caption{Contingency table of models with highest posterior model probability among three models calculated analytically and approximated using the resample-move particle filter with 100 particles.} \label{tab:cont-tab5}
\begin{tabular}{|c|c|c|c|}
\hline
 & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
 \hline
$M_1$ &   1 &   4 &   1 \\
\hline
$M_2$ &   6 &   4 &   0 \\
\hline
$M_3$ &   0 &   4 &   0 \\
 \hline
\end{tabular}
\end{center}
\end{table}

\begin{table}
\begin{center}
\caption{Contingency table of models with highest posterior model probability among three models calculated analytically and approximated using the resample-move particle filter with 500 particles.} \label{tab:cont-tab10}
\begin{tabular}{|c|c|c|c|}
\hline
 & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
 \hline
$M_1$ &  0  &  5  &  1 \\
\hline
$M_2$ &  6  &  4  &  0 \\
\hline
$M_3$ &  0  &  4  &  0 \\
 \hline
\end{tabular}
\end{center}
\end{table}

\begin{table}
\begin{center}
\caption{Contingency table of models with highest posterior model probability among three models calculated analytically and approximated using the resample-move particle filter with 1000 particles.} \label{tab:cont-tab15}
\begin{tabular}{|c|c|c|c|}
\hline
 & \multicolumn{3}{|c|}{PF approximation} \\
 \hline
 True Posterior & $M_1$& $M_2$ & $M_3$ \\
 \hline
$M_1$  &  0  &  5  &  1 \\
\hline
$M_2$  &  4  &  6  &  0 \\
\hline
$M_3$  &  0  &  4  &  0 \\
 \hline
\end{tabular}
\end{center}
\end{table}

\begin{figure}
\includegraphics[width=1.0\textwidth]{../graphs/10-23-13/mc_pf_test-ternary}
\caption{Ternary compositional plots of posterior model probabilities of $M_1$ ($W = 0.5$), $M_2$ ($W = 1$), and $M_3$ ($W = 2$) calculated analytically (top left) and approximated using the resample-move particle filter. Each point represents one simulated time series and the probability of a model generating a time series of observations is visualized by that point's proximity to that model's corner of the triangle. Points plotted in the same shade of gray correspond to the same simulation (to compare between plot panels).} \label{fig:ternary}
\end{figure}

\begin{figure}
\includegraphics[width=1.0\textwidth]{../graphs/10-23-13/mc_pf_test-binary}
\caption{Posterior model probabilities calculated analytically (left side of plot panels) and approximated using the resample-move particle filter (right side of plot panels) with varying numbers of particles (columns) when comparing $M_2$ against $M_1$ (top row) and $M_2$ against $M_3$ (bottom row). Calculations and particle filter runs were performed on 20 data sets simulated from $M_2$ with $\sigma^2 = 1$.} \label{fig:binary}
\end{figure}

\clearpage

\bibliographystyle{plainnat}
\bibliography{jarad}

\end{document} 