\documentclass{article}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{enumitem}

\begin{document}

Presented here is the particle filter known as the \emph{resample-move} algorithm, as described by \citet{Gilk:Berz:foll:2001} and \citet{Berz:gilk:2001}, applied to a dynamic linear model (DLM) \citep{petris2009dynamic}.

\section{Model} \label{sec:model}

We can represent a DLM as a state space model with observation and state equations given by

\begin{align}
y_t &= Fx_t + v_t \label{eqn:obs} \\
x_t &= Gx_{t-1} + w_t \label{eqn:state}
\end{align}

\noindent where $v_t$ and $w_t$ are iid Gaussian random variables with mean 0 and covariance matrices $V$ and $W$, respectively. We also assume $v_t \perp w_{t'}$ for all $t$ and $t'$. $y_t$ is the observed data at time $t$, and $x_t$ is the unknown state at time $t$. We regard $F$ and $G$ as known, and consider the special case where $V = \sigma^2V'$ and $W = \sigma^2W'$, where $V'$ and $W'$ are known and $\sigma^2$ is an unknown parameter. A fully Bayesian model is specified by defining the prior $p(x_0,\sigma^2)$, which we give as \[p(\sigma^2) \sim \mbox{IG}(a_0,b_0) \qquad p(x_0|\sigma^2) \sim \mbox{N}(0,\sigma^2C_0)\]
\noindent where $a_0$, $b_0$, and $C_0$ are known.

\section{Resample-move algorithm} \label{sec:rm}

Let $y_{1:t} = (y_1,y_2,\ldots,y_t)'$ represent the vector of data observed up until time $t$ and let $\theta_t$ be a vector of unknown states and/or parameters at time $t$. We allow $\theta_t$ to change with time, and we consider in particular the case where the dimension of $\theta_t$ changes with time, e.g. if $\theta_t$ contains an evolving trajectory of states. Let $\theta^{+}_t$ be the (possibly empty) set of unknown states entering the model at time $t+1$. Then, we can define $\theta_{t+1} = (\theta_t,\theta^{+}_t)'$. For the DLM described in Section \ref{sec:model}, we might wish to keep track of the entire trajectory of unknown states up to time $t$, in which case $\theta_t = (x_0,x_1,\ldots,x_t,\sigma^2)'$ and $\theta^{+}_t = x_{t+1}$.

We use particle filtering to approximate the posterior distribution $p(\theta_t|y_{1:t})$ through a weighted sample approximation, each sample being termed a \emph{particle}. Let $\theta_{j,t}$ be the vector of unknown states and/or parameters for particle $j$ at time $t$ and $w_{j,t}$ be the weight of particle $j$ at time $t$. We require that $\sum_{j=1}^J w_{j,t} = 1$, where $J$ is the total number of particles.

We initialize the algorithm by drawing $\theta_{j,0}$ from some prior distribution $p(\theta_0)$ and setting $w_{j,0} = 1 / J$ for $j = 1,\ldots,J$. Given a particle sample approximation of the posterior $p(\theta_{t-1}|y_{1:t-1})$ - i.e. $\left(\theta_{j,t-1}, w_{j,t-1}\right)$ for $j=1,\ldots,J$ - we move to a particle sample approximation of $p(\theta_t|y_{1:t})$ by the following steps:

\begin{enumerate}
\item Augmentation and update weights: For each $j = 1,\ldots,J$, draw $\theta^{+}_{j,t-1}$ from some state evolution distribution $p(\theta^{+}_{t-1}|\theta_{j,t-1})$ and define a new particle $\theta^{*}_{j,t} = (\theta_{j,t-1},\theta^{+}_{j,t-1})'$. Then, calculate the unnormalized weight $w^{**}_{j,t} = w_{j,t-1}p(y_t|\theta^{*}_{j,t-1})$.
\item Renormalize the $w^{**}_{j,t}$'s by setting $w^{*}_{j,t} = w^{**}_{j,t} / \sum_{l=1}^J w^{**}_{l,t}$.
\item Resample (optional): For each $j$, sample an index $k_j$ from $\{1,2,\ldots,J\}$ with associated probabilities $\{w^{*}_{1,t},w^{*}_{2,t},\ldots,w^{*}_{j,t},\ldots,w^{*}_{J,t}\}$.
\item {\tt if(}resampling was performed{\tt )\{} \\
\begin{enumerate}[label=\alph*.]
\item Update weights: Set $w_{j,t} = 1 / J$ for all $j$.
\item Move particles: For each $j = 1,2,\ldots,J$, draw $\theta_{j,t}$ from some transition kernel $q(\theta_t|\theta^{*}_{k_j,t})$ with invariant distribution $p(\theta_t|y_{1:t})$.
\end{enumerate}
{\tt \} else \{} Set $w_{j,t} = w^{*}_{j,t}$ and $\theta_{j,t} = \theta^{*}_{j,t}$ for all $j$.\}
\end{enumerate}

The `Resample' step is optional in the sense that if particle weights are not out of balance, it is not necessary to resample the particles because the weighted sample of particles would still provide a good approximation of the posterior. In this case, it would also not be necessary to move the particles, since the problem of degeneracy in the fixed parameters caused by resampling would not be present. By only resampling and moving particles at iterations of the particle filter for which it is necessary, we increase the efficiency of the algorithm. Measures of nonuniformity of particle weights such as effective sample size, coefficient of variation, and entropy can be used to determine when resampling should be performed \citep{Liu:Chen:Wong:reje:1998}. %We use an effective sample size threshold of 80\% of the total number of particles to determine when resample and move particles.

In addition, there are different ways to implement the `Resample' step that may be more efficient under certain conditions. The algorithm as written above implicity uses a \emph{multinomial} resampling scheme, i.e. when we say `sample an index $k_j$ ...'. Additional techniques for resampling exist including \emph{residual}, \emph{stratified}, and \emph{systematic} resampling. \citet{Douc:Capp:Moul:comp:2005} provides an overview of these methods. %We use stratified resampling.

\section{Model Comparison}

Using the algorithm described in Section \ref{sec:rm} we obtain an approximation to $p(\theta_t|y_{1:t})$ through $\sum_{j=1}^J \delta_{\theta_{j,t}}(x)$, where $\delta_{x_0}(x)$ is the Dirac delta mass function centered at $x_0$. In addition we can use the weighted particle sample to obtain an approximation to the marginal likelihood $p(y_{1:t})$. We first note that given $p(y_{1:t-1})$, $p(y_{1:t})$ can be updated recursively by $p(y_{1:t}) = p(y_t|y_{1:t-1})p(y_{1:t-1})$. Next, we note that an approximation of $p(y_t|y_{1:t-1})$ can be obtained from the weighted particle sample by 
\begin{equation}
p(y_t|y_{1:t-1}) \approx \sum_{j=1}^J w_{j,t-1}p(y_t|\theta_{j,t}) \label{eqn:condmarg}
\end{equation}

Having now prescribed a method for approximating $p(y_{1:t})$ sequentially within the resample-move algorithm, we can compare a set of possible models ${M_1,M_2,\ldots,M_p}$ be calculating posterior model probabilities by
\begin{equation}
p(M_i|y_{1:t}) = \frac{p(y_{1:t}|M_i)p(M_i)}{\sum_{i=1}^p p(y_{1:t}|M_i)p(M_i)} \label{eqn:modelcomp}
\end{equation}

\clearpage

\bibliographystyle{plainnat}
\bibliography{jarad}

\end{document} 