\documentclass{article}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}

\graphicspath{
{/Users/Danny/Documents/"UCSB - Research"/smc/graphs/},
{/Users/Danny/Documents/"UCSB - Research"/smc/graphs/twoEvent/}
}

\begin{document}

\section{Model} \label{sec:model}

We can represent a DLM as a state space model with observation and state equations given by

\begin{align}
y_t &= Fx_t + v_t \label{eqn:obs} \\
x_t &= Gx_{t-1} + w_t \label{eqn:state}
\end{align}

\noindent where $v_t$ and $w_t$ are iid Gaussian random variables with mean 0 and covariance matrices $V$ ($q \times q$) and $W$ ($p \times p$), respectively. We also assume $v_t \perp w_{t'}$ for all $t$ and $t'$. $y_t$ ($q \times 1$) is the observed data at time $t$, $x_t$ ($p \times 1$) is the unknown state at time $t$, and $F$ ($q \times p$) and $G$ ($p \times p$) are matrices that could contain unknown parameters. Lastly, we define the distribution of the prior state $x_0$ by $x_0 \sim \mbox{N}(m_0, C_0)$, where $\mbox{N}(\mu, \Sigma)$ represents the normal distribution with mean $\mu$ and covariance $\Sigma$.

We can allow additional covariates $U_t$ into the observation equation \eqref{eqn:obs} by including a term $U_t \beta$, where $U_t$ is a $q$ by $d$ matrix and $\beta$ is a $d$ by $1$ vector. In addition, we can allow $F$ to change over time. Thus, our DLM is now written as the following:

\begin{align}
y_t &= U_t\beta + F_tx_t + v_t \label{eqn:obs2} \\
x_t &= Gx_{t-1} + w_t \label{eqn:state2}
\end{align}

\noindent Letting $\beta = 0$, $U_t = 0$ for all $t$, and $F_t = F$ independent of $t$, we have the DLM described by equations \eqref{eqn:obs} and \eqref{eqn:state}.

\subsection{Regression with AR(p) errors} \label{sec:reg}

We will now consider a regression model with autoregressive error structure and separate observation and state noises. This specific model is a special case of the the DLM described in equations \eqref{eqn:obs2} and \eqref{eqn:state2}. In this case, $G$ has the form given by

\[G = \left(
\begin{array}{ccccc}
\phi_1 & 1 & 0 & \cdots & 0 \\
\phi_2 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & & \ddots & \vdots \\
\phi_{p-1} & 0 & \multicolumn{2}{c}{\cdots} & 1 \\
\phi_p & 0 & \multicolumn{2}{c}{\cdots} & 0
\end{array}
\right)\]

\noindent In addition, we let $W = \sigma^2_see'$ and $V = \sigma^2_m\tilde{V}$, where $e$ is a $p$-length vector with first element equal to 1 and the rest 0, $\sigma^2_s$ is the variance of the state (i.e. process) noise, and $\sigma^2_m$ is the variance of the observation (i.e. measurement) noise. We assume that the prior state $x_0$ is drawn from the stationary distribution given by $x_0 \sim \mbox{N}(m_0, \sigma^2_s\tilde{C_0})$, where \[\mbox{vec}({\tilde{C_0}}) = (I - G \otimes G)^{-1}\mbox{vec}(ee')\]

\section{Estimation}

Assume $\tilde{V}$, $U_t$, and $F_t$ known for all $t$. Let $\phi = (\phi_1,\ldots,\phi_p)'$, and let $\theta = (\beta', \phi', \sigma^2_m, \sigma^2_s)'$ be unknown. We now consider estimation of the joint posterior distribution of the states and unknown parameters using a Gibbs sampling algorithm. Define $y_{1:t} = (y_1,\ldots,y_t)'$ and $x_{0:t} = (x_0,x_1,\ldots,x_t)'$ to be the observed data and unobserved states, respectively, up until time $t$. We can express the joint likelihood $p(y_{1:t},x_{0:t},\theta)$ by the following general form:

\begin{equation}
p(y_{1:t},x_{0:t},\theta) = \prod_{k=1}^t\left[p(y_k|x_k,\theta)p(x_k|x_{k-1},\theta)\right]p(x_0|\theta)p(\theta) \label{eqn:lik}
\end{equation}

\noindent We use independent priors on the components of $\theta$, i.e. $p(\theta) = p(\beta)p(\phi)p(\sigma^2_m)p(\sigma^2_s)$, where

\begin{align*}
&\beta \sim \mbox{N}(b_0, B_0) \qquad \sigma^2_m \sim \mbox{IG}(a_{m_0}, b_{m_0})\\
&\phi \sim \mbox{N}_{S_\phi}(\phi_0, \Phi_0) \qquad \sigma^2_s \sim \mbox{IG}(a_{s_0}, b_{s_0})
\end{align*}

\noindent The hyperparameters $b_0$, $B_0$, $\phi_0$, $\Phi_0$, $a_{m_0}$, $b_{m_0}$, $a_{s_0}$, $b_{s_0}$, and $m_0$ are all assumed known. $\mbox{IG}(a,b)$ represents an inverse-gamma distribution with shape parameter $a$ and rate parameter $b$. $\mbox{N}_{\Omega}(\mu, \Sigma)$ represents the normal distribution truncated onto the set $\Omega$, with mean and covariance of its corresponding untruncated normal distribution given by $\mu$ and $\Sigma$, respectively. $S_{\phi}$ represents the set of all $\phi$ such that $x_t$ follows a stationary distribution.

To implement a Gibbs sampling algorithm that generates MCMC samples from the joint posterior distribution $p(x_{0:t},\theta|y_{1:t})$, we must draw from the following full conditional distributions given here in general and described in more detail in Section  \ref{sec:fullcond}:

\begin{align*}
p(\beta|y_{1:t},x_{0:t},\phi,\sigma^2_m,\sigma^2_s) &\propto p(y_{1:t},x_{0:t},\theta) \propto \prod_{k=1}^t\left[p(y_k|x_k,\beta,\sigma^2_m)\right]p(\beta) \\
p(\sigma^2_m|y_{1:t},x_{0:t},\beta,\phi,\sigma^2_s) &\propto p(y_{1:t},x_{0:t},\theta) \propto \prod_{k=1}^t\left[p(y_k|x_k,\beta,\sigma^2_m)\right]p(\sigma^2_m) \\
p(\phi|y_{1:t},x_{0:t},\beta,\sigma^2_m,\sigma^2_s) &\propto p(y_{1:t},x_{0:t},\theta) \propto \prod_{k=1}^t\left[p(x_k|x_{k-1},\phi,\sigma^2_s)\right]p(x_0|\phi,\sigma^2_s)p(\phi) \\
p(\sigma^2_s|y_{1:t},x_{0:t},\beta,\phi,\sigma^2_m) &\propto p(y_{1:t},x_{0:t},\theta) \propto \prod_{k=1}^t\left[p(x_k|x_{k-1},\phi,\sigma^2_s)\right]p(x_0|\phi,\sigma^2_s)p(\sigma^2_s) \\
p(x_{0:t}|y_{1:t},\theta) &\propto p(y_{1:t},x_{0:t},\theta) \propto \prod_{k=1}^t\left[p(y_k|x_k,\theta)p(x_k|x_{k-1},\theta)\right]p(x_0|\theta)
\end{align*}

\subsection{Full conditional distributions} \label{sec:fullcond}

We derive the following full conditional distributions for the unknown parameters:

\begin{itemize}
\item $\beta|y_{1:t},x_{0:t},\phi,\sigma^2_m,\sigma^2_s \sim \mbox{N}(b_t,B_t)$, where
\begin{align*}
b_t &= B_t\left(\frac{1}{\sigma^2_m} \sum^t_{k=1} U_k'\tilde{V}^{-1}(y_k - F_kx_k) + B_0^{-1}b_0\right) \\
B_t &= \left(\frac{1}{\sigma^2_m} \sum^t_{k=1} U_k'\tilde{V}^{-1}U_k + B_0^{-1}\right)^{-1}
\end{align*}
\item $\sigma^2_m|y_{1:t},x_{0:t},\beta,\phi,\sigma^2_s \sim \mbox{IG}(a_{m_t}, b_{m_t})$, where
\begin{align*}
a_{m_t} &= tq/2 + a_{m_0} \\
b_{m_t} &= \frac{1}{2}\mbox{SS}_y + b_{m_0} \\
\mbox{SS}_y &= \sum^t_{k=1} \left((y_k - U_k\beta - F_kx_k)'\tilde{V}^{-1}(y_k - U_k\beta - F_kx_k)\right)
\end{align*}
\item $p(\phi|y_{1:t},x_{0:t},\beta,\sigma^2_m,\sigma^2_s) = \Psi(\phi)\mbox{N}_{S_\phi}(\phi; \phi_t,\Phi_t)$, where
\begin{align*}
\phi_t &= \Phi_t\left(\frac{1}{\sigma^2_s} \tilde{X}'\tilde{x} + \Phi_0^{-1}\phi_0\right) \\
\Phi_t &= \left(\frac{1}{\sigma^2_s} \tilde{X}'\tilde{X} + \Phi_0^{-1}\right)^{-1} \\
\tilde{X} &= \left(\begin{array}{ccccc}
x_{0,1} & x_{-1,1} & \cdots & x_{1-p,1} \\
x_{1,1} & x_{0,1} & \cdots & x_{2-p,1} \\
\vdots & \vdots & & \vdots \\
x_{t-1,1} & x_{t-1,1} & \cdots & x_{t-p,1}
\end{array} \quad
\right) \tilde{x} = \left(\begin{array}{c} x_{1,1} \\ x_{2,1} \\ \vdots \\ x_{t,1} \end{array}\right)
\mbox{ with } x_k = \left(\begin{array}{c} x_{k,1} \\ x_{k,2} \\ \vdots \\ x_{k,p} \end{array}\right) \\
\Psi(\phi) &= \left|\tilde{C_0}\right|^{-1/2}\exp\left(-\frac{1}{2\sigma^2_s}(x_0-m_0)'\tilde{C_0}^{-1}(x_0-m_0)\right)
\end{align*}
\noindent and $\mbox{N}_{\Omega}(.;\mu,\Sigma)$ represents the pdf of the normal distribution truncated onto the set $\Omega$, with mean and covariance of its corresponding untruncated normal distribution given by $\mu$ and $\Sigma$, respectively. If $p > 1$, the pre-sample states $\{x_{-1,1}, x_{-2,1}, \ldots, x_{-(p-1),1}\}$ need to be calculated in order to construct $\tilde{X}$. These can be obtained by the following recursive procedure: \\
\\
For each $k \in \{1,2,\ldots,p-1\}$
\begin{enumerate}
\item Calculate $x_{-k,1} = x_{1-k,p} / \phi_p$. If $k = p - 1$, stop. Otherwise, proceed to Step \ref{step:recurse}.
\item For each $j \in \{p,p-1,\ldots,k+2\}$, calculate $x_{-k,j} = x_{1-k,j-1} - \phi_{j-1}x_{-k,1}$. \label{step:recurse}
\end{enumerate}
\item $\sigma^2_s|y_{1:t},x_{0:t},\beta,\phi,\sigma^2_m \sim \mbox{IG}(a_{s_t}, b_{s_t})$, where
\begin{align*}
a_{s_t} &= (p/2)(t+1) + a_{s_0} \\
b_{s_t} &= \frac{1}{2}\mbox{SS}_x + \frac{1}{2}(x_0-m_0)'\tilde{C_0}^{-1}(x_0-m_0) + b_{s_0} \\
\mbox{SS}_x &= (\tilde{x} - \tilde{X}\phi)'(\tilde{x} - \tilde{X}\phi)
\end{align*}
\end{itemize}

The full conditional distribution of the states, $p(x_{0:t}|y_{1:t},\theta)$, can be calculated by
\[p(x_{0:t}|y_{1:t},\theta) = \prod_{k=0}^t p(x_k|y_{1:k},x_{k+1},\theta)\]
where the last product is simply $p(x_t|y_{1:t},\theta)$. The distributions $p(x_k|y_{1:k},x_{k+1},\theta)$ for $k = 0,1,\ldots,t$ can be computed using the Kalman filter and smoothing recursions \citep{petris2009dynamic}, described in detail in Section \ref{sec:gibbs}.

\subsection{Gibbs sampler} \label{sec:gibbs}

We generate samples from the joint posterior distribution $p(x_{0:t},\theta|y_{1:t})$ using the following MCMC algorithm:

\begin{enumerate}
\item Set $i = 0$ and draw initial values $x_{0:t}^{(i)} = \left(x_0^{(i)},x_1^{(i)},\ldots,x_t^{(i)}\right)'$ and $\theta^{(i)} = \left({\beta^{(i)}}', {\phi^{(i)}}', {\sigma^2_m}^{(i)}, {\sigma^2_s}^{(i)}\right)$. Set $i = 1$.
\item Sample $\beta^{(i)}|\cdots \sim \mbox{N}(b_n^{(i)}, B_n^{(i)})$, where
\begin{align*}
b_n^{(i)} &= B_n^{(i)}\left(\frac{1}{{\sigma^2_m}^{(i-1)}} \sum^t_{k=1} U_k'\tilde{V}^{-1}(y_k - F_kx_k^{(i-1)}) + B_0^{-1}b_0\right) \\
B_n^{(i)} &= \left(\frac{1}{{\sigma^2_m}^{(i-1)}} \sum^t_{k=1} U_k'\tilde{V}^{-1}U_k + B_0^{-1}\right)^{-1}
\end{align*}
\item Sample ${\sigma^2_m}^{(i)}|\cdots \sim \mbox{IG}(a_{m_t}^{(i)}, b_{m_t}^{(i)})$, where
\begin{align*}
a_{m_t}^{(i)} &= tq/2 + a_{m_0} \\
b_{m_t}^{(i)} &= \frac{1}{2}\mbox{SS}_y + b_{m_0} \\
\mbox{SS}_y &= \sum^t_{k=1} \left((y_k - U_k\beta^{(i)} - F_kx_k^{(i-1)})'\tilde{V}^{-1}(y_k - U_k\beta^{(i)} - F_kx_k^{(i-1)})\right)
\end{align*}
\item Sample $\phi^{(i)}$ using the following Metropolis-Hastings step:
\begin{enumerate}
\item Draw $\phi^* \sim \mbox{N}(\phi_t^{(i)},\Phi_t^{(i)})$, where
\begin{align*}
\phi_t &= \Phi_t\left(\frac{1}{\sigma^2_s} \tilde{X}'\tilde{x} + \Phi_0^{-1}\phi_0\right) \\
\Phi_t &= \left(\frac{1}{\sigma^2_s} \tilde{X}'\tilde{X} + \Phi_0^{-1}\right)^{-1}
\end{align*} \label{step:phi}
\item If $\phi^* \in S_{\phi}$, go to Step \ref{step:mh}. Otherwise, go back to step \ref{step:phi}.
\item Calculate $R = \mbox{min}\left\{\frac{\Psi(\phi*)}{\Psi(\phi^{(i-1)})}, 1\right\}$ and draw $u \sim \mbox{Unif}[0,1]$. If $u < R$, set $\phi^{(i)} = \phi^*$. Otherwise, set $\phi^{(i)} = \phi^{(i-1)}$. \label{step:mh}
\end{enumerate}
\item Sample ${\sigma^2_s}^{(i)}|\cdots \sim \mbox{IG}(a_{s_t}^{(i)}, b_{s_t}^{(i)})$, where
\begin{align*}
a_{s_t}^{(i)} &= (p/2)(t+1) + a_{s_0} \\
b_{s_t}^{(i)} &= \frac{1}{2}\mbox{SS}_x + \frac{1}{2}(x_0^{(i-1)}-m_0)'\tilde{C_0}(\phi^{(i)})(x_0^{(i-1)}-m_0) + b_{s_0} \\
\mbox{SS}_x &= (\tilde{x} - \tilde{X}\phi)'(\tilde{x} - \tilde{X}\phi)
\end{align*}
\noindent Here, $\tilde{C_0}(\phi^{(i)})$ implies that the matrix $\tilde{C_0}$ is constructed using $\phi^{(i)}$.
\item Sample $x_{0:t}^{(i)}$ using the following forward filtering backward sampling (FFBS) algorithm \citep{petris2009dynamic}:
\begin{enumerate}
\item Starting with initial values $m_0$ and $C_0 = {\sigma^2_s}^{(i)}\tilde{C_0}$, calculate the quantities $a_k$, $R_k$, $f_k$, $Q_k$, $m_k$, and $C_k$ for $k = 1,2,\ldots,t$ using the following recursive equations (known as the Kalman filter):
    \begin{align*}
    a_k &= Gm_{k-1} &\qquad R_k &= GC_{k-1}G' + W^{(i)} \\
    f_k &= U_k\beta^{(i)} + F_ka_k &\qquad Q_k &= F_kR_kF_k' + V^{(i)} \\
    m_k &= a_k + R_kF_k'Q_k^{-1}(y_k-f_k) &\qquad C_k &= R_k - R_kF_k'Q_k^{-1}F_kR_k
    \end{align*}
    \noindent where $W^{(i)} = {\sigma^2_s}^{(i)}ee'$ and $V^{(i)} = {\sigma^2_m}^{(i)}\tilde{V}$.
\item Draw $x_t^{(i)} \sim \mbox{N}(m_t,C_t)$. Then, for $k = t-1,\ldots,0$, draw $x_k^{(i)} \sim \mbox{N}(h_k,H_k)$, where
    \begin{align*}
    h_k &= m_k + C_kG'R_{k+1}^{-1}(x_{k+1}^{(i)} - a_{k+1}) \\
    H_k &= C_k - C_kG'R_{k+1}^{-1}GC_k
    \end{align*}
\end{enumerate}
\end{enumerate}

\section{Example - dynamic regression} \label{sec:exdr}

Consider a time series from one voxel of the brain in an fMRI experiment. We could visualize this time series as begin generated from a dynamic regression model, where the dynamic slope follows an AR(1) process, i.e. a model of the form described by equations \eqref{eqn:obs2} and \eqref{eqn:state2} where:

\begin{equation}
U_t = (1, \mbox{conv}_t) \quad \beta = (\beta_0,\beta_1)' \quad F_t = (\mbox{conv}_t) \quad G = \phi \label{eqn:dr}
\end{equation}

\noindent Here, $\mbox{conv}_t$ is the convolution of the hemodynamic response function with the experimental design, giving the expected BOLD response if the experimental stimulus triggers neural activation. Notice that multiplying out the observation equation yields the following state-space model:
\begin{align*}
y_t &= \beta_0 + (\beta_1 + x_t)\mbox{conv}_t + v_t \\
x_t &= \phi x_{t-1} + w_t
\end{align*}
\noindent where $v_t \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_m\tilde{V})$ and $w_t \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_s)$.
Thus, $\beta_0$ represents the baseline level of BOLD signal, $\beta_1$ is the mean level of subject response to the experimental stimulus, and $x_t$ is the change in subject response from the mean at time $t$. $x_t$ can reflect changes in focus of the subject in the scanner over time, and is specified by the state equation to follow an AR(1) process.

A time series of length $T = 245$ was simulated from a dynamic regression with the following parameter values:
\begin{align*}
\beta_0 = 900 &\quad \beta_1 = 5 \\
\tilde{V} = 1 &\quad \sigma_m^2 = 1 \\
\phi = 0.9 &\quad \sigma_s^2 = 1
\end{align*}
Three MCMC chains using the Gibbs sampling algorithm described in Section \ref{sec:gibbs} were run for 11000 iterations with a burn-in period of 1000. Samples from every 10th iteration of the Gibbs sampler were saved, yielding the following marginal traceplots for the fixed parameters shown in Figure \ref{fig:tracetheta} and $x_t$ for $t \in \{0, 30, 61, 91, 122, 153, 183, 214, 245\}$ shown in Figure \ref{fig:tracex}. Maximum likelihood estimates shown in the figures were obtained using the {\tt dlm} package in R \citep{petris2009dynamic}.

\begin{figure}[ht]
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-2-3-11000-1000-10-FALSE-traceplots-beta}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-2-3-11000-1000-10-FALSE-traceplots-phi-1}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-2-3-11000-1000-10-FALSE-traceplots-sigma2m}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-2-3-11000-1000-10-FALSE-traceplots-sigma2s-1}
\end{minipage}
\caption{Marginal traceplots for unknown parameters of a dynamic regression model for three MCMC chains (colors). Gray horizontal lines correspond to MLEs and black horizontal lines correspond to true values used for simulation with actual values printed along the right side of the plot panel.} \label{fig:tracetheta}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-2-3-11000-1000-10-FALSE-traceplots-x-1}
\caption{Marginal traceplots for the change in slope $x_t$ of a dynamic regression model for three MCMC chains (colors) for different $t$ (plot panels). Gray horizontal lines correspond to smoothed estimates with fixed parameter values set at the MLEs, and black horizontal lines correspond to true simulated states with actual values printed along the right side of the plot panel.} \label{fig:tracex}
\end{figure}

\clearpage

\section{Example - Regression with AR(1)+WN errors} \label{sec:exar}

We can also visualize fMRI data being generated from a regression model with a constant slope and autocorrelated errors. The dynamic regression model specified by equation \eqref{eqn:dr} can be changed to this model by letting $F_t = 1$ instead of $\mbox{conv}_t$. That is, we can form a DLM representation of the following model:
\begin{align*}
y_t &= \beta_0 + \beta_1\mbox{conv}_t + x_t + v_t \\
x_t &= \phi x_{t-1} + w_t
\end{align*}
\noindent where $v_t \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_m\tilde{V})$ and $w_t \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_s)$. A time series were generated from this model with parameter values set to
\begin{align*}
\beta_0 = 900 &\quad \beta_1 = 5 \\
\tilde{V} = 1 &\quad \sigma_m^2 = 0.5 \\
\phi = 0.4 &\quad \sigma_s^2 = 1
\end{align*}

Three MCMC chains were run on the simulated data set in the same manner as those run for the dynamic regression model in Section \ref{sec:exdr}. Figures \ref{fig:tracetheta-arwn} and \ref{fig:tracex-arwn} shows traceplots for the fixed parameters and states from the MCMC chains.

\begin{figure}[ht]
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M011-2-3-11000-1000-10-FALSE-traceplots-beta}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M011-2-3-11000-1000-10-FALSE-traceplots-phi-1}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M011-2-3-11000-1000-10-FALSE-traceplots-sigma2m}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M011-2-3-11000-1000-10-FALSE-traceplots-sigma2s-1}
\end{minipage}
\caption{Marginal traceplots for unknown parameters of a regression model with AR(1)+WN errors for three MCMC chains (colors). Gray horizontal lines correspond to MLEs and black horizontal lines correspond to true values used for simulation with actual values printed along the right side of the plot panel.} \label{fig:tracetheta-arwn}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M011-2-3-11000-1000-10-FALSE-traceplots-x-1}
\caption{Marginal traceplots for the change in slope $x_t$ of a regression model with AR(1)+WN errors for three MCMC chains (colors) for different $t$ (plot panels). Gray horizontal lines correspond to smoothed estimates with fixed parameter values set at the MLEs, and black horizontal lines correspond to true simulated states with actual values printed along the right side of the plot panel.} \label{fig:tracex-arwn}
\end{figure}

\clearpage

\section{Example - dynamic regression extended} \label{sec:drext}

In this section, we add an additional dynamic regression coefficient to the model and assume that the two regression coefficients, e.g. $\beta_1$ and $\beta_2$, change over time in the exact same manner. That is, we assume the following model:
\begin{align*}
y_t &= \beta_0 + (\beta_1 + x_t)\mbox{conv}_{1,t} + (\beta_2 + x_t)\mbox{conv}_{2,t} + v_t \\
x_t &= \phi x_{t-1} + w_t
\end{align*}
\noindent where $\mbox{conv}_{1,t}$ and $\mbox{conv}_{2,t}$ now represent the predicted BOLD response to two separate events. A time series of length $T = 250$ was simulated from this model with the following parameter values:
\begin{align*}
\beta_0 = 900 &\quad \beta_1 = 5 \quad \beta_2 = 2 \\
\tilde{V} = 1 &\quad \sigma_m^2 = 1 \\
\phi = 0.9 &\quad \sigma_s^2 = 1
\end{align*}
Three MCMC chains using the Gibbs sampling algorithm described in Section \ref{sec:gibbs} were run for 11000 iterations with a burn-in period of 1000. Samples from every iteration of the Gibbs sampler were saved, yielding the following marginal traceplots for the fixed parameters shown in Figure \ref{fig:tracetheta-drext} and $x_t$ for $t \in \{0, 31, 62, 93, 125, 156, 187, 218, 250\}$ shown in Figure \ref{fig:tracex-drext}. Maximum likelihood estimates shown in the figures were obtained using the {\tt dlm} package in R \citep{petris2009dynamic}.

\begin{figure}[ht]
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-3-3-11000-1000-1-FALSE-traceplots-beta}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-3-3-11000-1000-1-FALSE-traceplots-phi-1}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-3-3-11000-1000-1-FALSE-traceplots-sigma2m}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-3-3-11000-1000-1-FALSE-traceplots-sigma2s-1}
\end{minipage}
\caption{Marginal traceplots for unknown parameters of the two-coefficient dynamic regression model with the same time-evolving state for three MCMC chains (colors). Gray horizontal lines correspond to MLEs and black horizontal lines correspond to true values used for simulation with actual values printed along the right side of the plot panel.} \label{fig:tracetheta-drext}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-3-3-11000-1000-1-FALSE-traceplots-x-1}
\caption{Marginal traceplots for the change in slopes $x_t$ of the two-coefficient dynamic regression model for three MCMC chains (colors) for different $t$ (plot panels). Gray horizontal lines correspond to smoothed estimates with fixed parameter values set at the MLEs, and black horizontal lines correspond to true simulated states with actual values printed along the right side of the plot panel.} \label{fig:tracex-drext}
\end{figure}

\clearpage

\section{Adding more autoregressive components} \label{sec:add}

We can extend the general form of regression models with one autoregressive error component described in Section \ref{sec:reg} to include regression models with multiple autoregressive components. For example, consider a model of the form \[y_t = U_t\beta + F_tx_t + v_t\] where $x_t = (x_{t,1},x_{t,2})'$, $x_{t,1}$ follows an $\mbox{AR(}p_1\mbox{)}$ process, and $x_{t,2}$ follows an $\mbox{AR(}p_2\mbox{)}$ process. We can describe the general form for a regression model with $d$ independent autoregressive error components by the DLM from equations \eqref{eqn:obs2} and \eqref{eqn:state2} by letting $x_t$ be a $P = p_1 + p_2 + \cdots + p_d$ length vector, $\tilde{\phi}_i = (\phi_{i,1},\phi_{i,2},\ldots,\phi_{i,p_i})'$ the AR coefficients for the $i^{\mbox{th}}$ autoregressive error term, $\sigma^2_{s_1}, \sigma^2_{s_2}, \ldots, \sigma^2_{s_d}$ the $d$ white noise variances for each AR component, and $G$ a $P \times P$ block-diagonal matrix with $i^{\mbox{th}}$ block

\[G_i = \left(
\begin{array}{ccccc}
\phi_{i,1} & 1 & 0 & \cdots & 0 \\
\phi_{i,2} & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & & \ddots & \vdots \\
\phi_{i,p_i-1} & 0 & \multicolumn{2}{c}{\cdots} & 1 \\
\phi_{i,p_i} & 0 & \multicolumn{2}{c}{\cdots} & 0
\end{array}
\right).\]

\noindent $W$ is also a $P\times P$ block diagonal matrix with $i^{\mbox{th}}$ block $W_i = \sigma^2_{s_i}e_ie_i'$ where $e_i$ is a $p_i$-length vector with first element equal to 1 and the rest 0. $V = \sigma^2_m$ is defined the same way as in Section  \ref{sec:reg}, and the prior state $x_0$ is drawn from the same stationary distribution given in Section \ref{sec:reg} where $\tilde{C}_0$ is now a $P\times P$ block diagonal matrix with $i^{\mbox{th}}$ block $\tilde{C}_{0_i}$ satisfying \[\mbox{vec}(\tilde{C}_{0_i}) = (I - G_i \otimes G_i)^{-1}\mbox{vec}(e_ie_i')\]

We can estimate $\tilde{\phi}_i$ and $\sigma^2_{s_i}$ for $i = 1,2,\ldots,d$ using the Gibbs sampling algorithm described in Section \ref{sec:gibbs} with a slight variation to add the full conditional distributions for $\tilde{\phi}_i$ and $\sigma^2_{s_i}$. Let $c_i = 1 + p_1 + p_2 + \cdots p_{i-1}$ for $i = 1,2,\ldots,d,d+1$ with the convention that $c_1 = 1$ and $c_{d+1} = P$, and $x_{k,j}$ the $j^{\mbox{th}}$ element of $x_k$. Given prior distributions $\tilde{\phi}_i \sim \mbox{N}_{S_{\tilde{\phi}_i}}(\tilde{\phi}_{0,i}, \Phi_{0,i})$ and $\sigma^2_{s_i} \sim \mbox{IG}(a^i_{s_0}, b^i_{s_0})$ for $i = 1,2,\ldots,d$, the full conditionals are given by
\begin{itemize}
\item $p(\tilde{\phi}_i|\cdots) = \Psi(\phi)\mbox{N}_{S_{\tilde{\phi}_i}}(\tilde{\phi}_i; \tilde{\phi}_{t_i},\Phi_{t_i})$, where
\begin{align*}
\tilde{\phi}_{t_i} &= \Phi_t\left(\frac{1}{\sigma^2_{s,i}} \tilde{X}_i'\tilde{x}_i + \Phi_{0_i}^{-1}\tilde{\phi}_{0,i}\right) \\
\Phi_{t,i} &= \left(\frac{1}{\sigma^2_{s,i}} \tilde{X}_i'\tilde{X}_i + \Phi_{0,i}^{-1}\right)^{-1} \\
\tilde{X}_i &= \left(\begin{array}{ccccc}
x_{0,c_i} & x_{-1,c_i} & \cdots & x_{1-p_i,c_i} \\
x_{1,c_i} & x_{0,c_i} & \cdots & x_{2-p_i,c_i} \\
\vdots & \vdots & & \vdots \\
x_{t-1,c_i} & x_{t-1,c_i} & \cdots & x_{t-p_i,c_i}
\end{array} \quad
\right) \tilde{x}_i = \left(\begin{array}{c} x_{1,c_i} \\ x_{2,c_i} \\ \vdots \\ x_{t,c_i} \end{array}\right) \\
\Psi(\tilde{\phi}_i) &= \left|\tilde{C}^i_0\right|^{-1/2}\exp\left(-\frac{1}{2\sigma^2_{s,i}}(x_{0,c_i:c_{i+1}-1}-m_{0,c_i:c_{i+1}-1})'\tilde{C}_{0_i}^{-1}(x_{0,c_i:c_{i+1}-1}-m_{0,c_i:c_{i+1}-1})\right),
\end{align*}
where $x_{0,j:h}$ and $m_{0,j:h}$ denote elements $j$ through $h$ of $x_0$ and $m_0$, respectively. As was the case with the regression model with just one AR error term, if $p_i > 1$, the pre-sample states $\{x_{-1,c_i}, x_{-2,c_i}, \ldots, x_{-(p-1),c_i}\}$ need to be calculated in order to construct $\tilde{X}_i$. These can be obtained using the same recursive procedure specified for the full conditional of $\phi$ in Section \ref{sec:fullcond} setting $x_0 = (x_{0,c_i},x_{0,c_i+1},\ldots,x_{0,c_i+p_i-1})'$, $p = p_i$, and $\phi_j = \phi_{i,j}$ for $j = 1,\ldots,p$. \\
\item $\sigma^2_{s,i}|\cdots \sim \mbox{IG}(a^i_{s_t}, b^i_{s_t})$, where
\begin{align*}
a^i_{s_t} &= (p_i/2)(t+1) + a^i_{s_0} \\
b^i_{s_t} &= \frac{1}{2}\mbox{SS}^i_x + \frac{1}{2}(x_{0,c_i:c_{i+1}-1}-m_{0,c_i:c_{i+1}-1})'\tilde{C}_{0,i}^{-1}(x_{0,c_i:c_{i+1}-1}-m_{0,c_i:c_{i+1}-1}) + b^i_{s_0} \\
\mbox{SS}^i_x &= (\tilde{x}_i - \tilde{X}_i\tilde{\phi}_i)'(\tilde{x}_i - \tilde{X}_i\tilde{\phi}_i)
\end{align*}
\end{itemize}

\noindent We can also consider the special case of multiple AR components following the same AR process, i.e. $\phi = \tilde{\phi}_1 = \tilde{\phi}_2 = \cdots = \tilde{\phi}_d$ and $\sigma^2_s = \sigma^2_{s,1} = \sigma^2_{s,2} = \cdots = \sigma^2_{s,d}$. In this case, the full conditionals of $\phi$ and $\sigma^2_s$ are given by
\begin{itemize}
\item $p(\phi|\cdots) = \Psi(\phi)\mbox{N}_{S_\phi}(\phi; \phi_t,\Phi_t)$, where
\begin{align*}
\phi_t &= \Phi_t\left(\frac{1}{\sigma^2_s} \sum_{k=1}^t \tilde{X}_k'\tilde{x}_k + \Phi_0^{-1}\phi_0\right) \\
\Phi_t &= \left(\frac{1}{\sigma^2_s} \sum_{k=1}^t \tilde{X}_k'\tilde{X}_k + \Phi_0^{-1}\right)^{-1} \\
\tilde{X}_k &= \left(\begin{array}{ccccc}
x_{k-1,c_1} & x_{k-2,c_1} & \cdots & x_{k-p,c_1} \\
x_{k-1,c_2} & x_{k-2,c_2} & \cdots & x_{k-p,c_2} \\
\vdots & \vdots & & \vdots \\
x_{k-1,c_d} & x_{k-2,c_d} & \cdots & x_{k-p,c_d}
\end{array} \quad
\right) \tilde{x}_k = \left(\begin{array}{c} x_{k,c_1} \\ x_{k,c_2} \\ \vdots \\ x_{k,c_d} \end{array}\right) \\
\Psi(\phi) &= \left|\tilde{C_0}\right|^{-1/2}\exp\left(-\frac{1}{2\sigma^2_s}(x_0-m_0)'\tilde{C_0}^{-1}(x_0-m_0)\right)
\end{align*}
\item $\sigma^2_s|\cdots \sim \mbox{IG}(a_{s_t}, b_{s_t})$, where
\begin{align*}
a_{s_t} &= (pd/2)(t+1) + a_{s_0} \\
b_{s_t} &= \frac{1}{2}\mbox{SS}_x + \frac{1}{2}(x_0-m_0)'\tilde{C_0}^{-1}(x_0-m_0) + b_{s_0} \\
\mbox{SS}_x &= \sum_{k=1}^t \left( (\tilde{x}_k - \tilde{X}_k\phi)'(\tilde{x}_k - \tilde{X}_k\phi) \right)
\end{align*}
\end{itemize}

\section{Example - dynamic regression with change in slopes following different AR(1) processes}

In this section, we assume that the two coefficients of a dynamic regression model, e.g. $\beta_1$ and $\beta_2$, evolve over time in different manners according to different AR(1) processes. That is, we assume the following model:
\begin{align*}
y_t &= \beta_0 + (\beta_1 + x_{1,t})\mbox{conv}_{1,t} + (\beta_2 + x_{2,t})\mbox{conv}_{2,t} + v_t \\
x_{1,t} &= \phi_1 x_{1,t-1} + w_{1,t} \\
x_{2,t} &= \phi_2 x_{2,t-1} + w_{2,t}
\end{align*}
\noindent where $x_t = (x_{1,t}, x_{2,t})'$ now represents the change in the first and second regression coefficients at time $t$, $v_t \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_m\tilde{V})$, $w_{1,t} \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_{s,1})$, $w_{2,t} \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_{s,2})$. A DLM representation of this model can be formed by letting

\begin{equation}
U_t = (1, \mbox{conv}_{1,t}, \mbox{conv}_{2,t}) \quad \beta = (\beta_0,\beta_1,\beta_2)' \quad F_t = (\mbox{conv}_{1,t}, \mbox{conv}_{2,t}) \quad G = \left( \begin{array}{cc} \phi & 0 \\ 0 & \phi \end{array} \right) \label{eqn:dr}
\end{equation}

A time series of length $T = 250$ was simulated from this model with the following parameter values:
\begin{align*}
\beta_0 = 900 &\quad \beta_1 = 5 \quad \beta_2 = 2 \\
\tilde{V} = 1 &\quad \sigma_m^2 = 1 \\
\phi_1 = 0.99 &\quad \sigma^2_{s,1} = 1.5 \\
\phi_2 = 0.7 &\quad \sigma^2_{s,2} = 0.5
\end{align*}
Three MCMC chains using the Gibbs sampling algorithm described in Section \ref{sec:gibbs} were run for 11000 iterations with a burn-in period of 1000. Samples from every iteration of the Gibbs sampler were saved, yielding the following marginal traceplots for the fixed parameters shown in Figure \ref{fig:tracetheta-drdiff} and $x_t$ for $t \in \{0, 31, 62, 93, 125, 156, 187, 218, 250\}$ shown in Figures \ref{fig:tracex-drdiff1} and \ref{fig:tracex-drdiff2}. Maximum likelihood estimates shown in the figures were obtained using the {\tt dlm} package in R \citep{petris2009dynamic}.

\begin{figure}[ht]
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-diff-3-3-11000-1000-1-FALSE-traceplots-beta}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-diff-3-3-11000-1000-1-FALSE-traceplots-phi-1}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-diff-3-3-11000-1000-1-FALSE-traceplots-phi-2}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-diff-3-3-11000-1000-1-FALSE-traceplots-sigma2m}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-diff-3-3-11000-1000-1-FALSE-traceplots-sigma2s-1}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-diff-3-3-11000-1000-1-FALSE-traceplots-sigma2s-2}
\end{minipage}
\caption{Marginal traceplots for unknown parameters of the two-coefficient dynamic regression model with different time-evolving states following different AR(1) processes for three MCMC chains (colors). Gray horizontal lines correspond to MLEs and black horizontal lines correspond to true values used for simulation with actual values printed along the right side of the plot panel.} \label{fig:tracetheta-drdiff}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-diff-3-3-11000-1000-1-FALSE-traceplots-x-1}
\caption{Marginal traceplots for the change in slope $x_{1,t}$ of the two-coefficient dynamic regression model with different time-evolving states following different AR(1) processes for three MCMC chains (colors) for different $t$ (plot panels). Gray horizontal lines correspond to smoothed estimates with fixed parameter values set at the MLEs, and black horizontal lines correspond to true simulated states with actual values printed along the right side of the plot panel.} \label{fig:tracex-drdiff1}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-6-1-M101-diff-3-3-11000-1000-1-FALSE-traceplots-x-2}
\caption{Marginal traceplots for the change in slope $x_{2,t}$ of the two-coefficient dynamic regression model with different time-evolving states following different AR(1) processes for three MCMC chains (colors) for different $t$ (plot panels). Gray horizontal lines correspond to smoothed estimates with fixed parameter values set at the MLEs, and black horizontal lines correspond to true simulated states with actual values printed along the right side of the plot panel.} \label{fig:tracex-drdiff2}
\end{figure}

\clearpage

\section{Example - dynamic regression with change in slopes following the same AR(1) process}

In this section, we assume that the two coefficients of a dynamic regression model, e.g. $\beta_1$ and $\beta_2$, evolve over time in different manners according to the same AR(1) process. That is, we assume the following model:
\begin{align*}
y_t &= \beta_0 + (\beta_1 + x_{1,t})\mbox{conv}_{1,t} + (\beta_2 + x_{2,t})\mbox{conv}_{2,t} + v_t \\
x_{1,t} &= \phi x_{1,t-1} + w_t \\
x_{2,t} &= \phi x_{2,t-1} + w_t
\end{align*}
\noindent where $x_t = (x_{1,t}, x_{2,t})'$ again represents the change in the first and second regression coefficients at time $t$, $v_t \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_m\tilde{V})$ and $w_t \stackrel{iid}{\sim} \mbox{N}(0,\sigma^2_s)$. A DLM representation of this model can be formed by letting

\begin{equation}
U_t = (1, \mbox{conv}_{1,t}, \mbox{conv}_{2,t}) \quad \beta = (\beta_0,\beta_1,\beta_2)' \quad F_t = (\mbox{conv}_{1,t}, \mbox{conv}_{2,t}) \quad G = \left( \begin{array}{cc} \phi_1 & 0 \\ 0 & \phi \end{array} \right) \label{eqn:dr}
\end{equation}

A time series of length $T = 250$ was simulated from this model with the following parameter values:
\begin{align*}
\beta_0 = 900 &\quad \beta_1 = 5 \quad \beta_2 = 2 \\
\tilde{V} = 1 &\quad \sigma_m^2 = 1 \\
\phi = 0.99 &\quad \sigma^2_{s,1} = 0.5
\end{align*}
Three MCMC chains using the Gibbs sampling algorithm described in Section \ref{sec:gibbs} were run for 11000 iterations with a burn-in period of 1000. Samples from every iteration of the Gibbs sampler were saved, yielding the following marginal traceplots for the fixed parameters shown in Figure \ref{fig:tracetheta-drdiffsame} and $x_t$ for $t \in \{0, 31, 62, 93, 125, 156, 187, 218, 250\}$ shown in Figures \ref{fig:tracex-drdiffsame1} and \ref{fig:tracex-drdiffsame2}. Maximum likelihood estimates shown in the figures were obtained using the {\tt dlm} package in R \citep{petris2009dynamic}.

\begin{figure}[ht]
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-4-1-M101-diff-3-3-11000-1000-1-TRUE-traceplots-beta}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-4-1-M101-diff-3-3-11000-1000-1-TRUE-traceplots-phi-1}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-4-1-M101-diff-3-3-11000-1000-1-TRUE-traceplots-sigma2m}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-4-1-M101-diff-3-3-11000-1000-1-TRUE-traceplots-sigma2s-1}
\end{minipage}
\caption{Marginal traceplots for unknown parameters of the two-coefficient dynamic regression model with different time-evolving states following the same AR(1) process for three MCMC chains (colors). Gray horizontal lines correspond to MLEs and black horizontal lines correspond to true values used for simulation with actual values printed along the right side of the plot panel.} \label{fig:tracetheta-drdiffsame}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-4-1-M101-diff-3-3-11000-1000-1-TRUE-traceplots-x-1}
\caption{Marginal traceplots for the change in slope $x_{1,t}$ of the two-coefficient dynamic regression model with different time-evolving states following the same AR(1) process for three MCMC chains (colors) for different $t$ (plot panels). Gray horizontal lines correspond to smoothed estimates with fixed parameter values set at the MLEs, and black horizontal lines correspond to true simulated states with actual values printed along the right side of the plot panel.} \label{fig:tracex-drdiffsame1}
\end{figure}

\begin{figure}[ht]
\includegraphics[width=1.0\textwidth]{fmri_dlm_mcmc_test-20-4-1-M101-diff-3-3-11000-1000-1-TRUE-traceplots-x-2}
\caption{Marginal traceplots for the change in slope $x_{2,t}$ of the two-coefficient dynamic regression model with different time-evolving states following the same AR(1) process for three MCMC chains (colors) for different $t$ (plot panels). Gray horizontal lines correspond to smoothed estimates with fixed parameter values set at the MLEs, and black horizontal lines correspond to true simulated states with actual values printed along the right side of the plot panel.} \label{fig:tracex-drdiffsame2}
\end{figure}

\clearpage

\bibliographystyle{plainnat}
\bibliography{jarad}

\end{document} 