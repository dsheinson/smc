\documentclass{article}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}

\begin{document}

\section{Model} \label{sec:model}

We can represent a DLM as a state space model with observation and state equations given by

\begin{align}
y_t &= Fx_t + v_t \label{eqn:obs} \\
x_t &= Gx_{t-1} + w_t \label{eqn:state}
\end{align}

\noindent where $v_t$ and $w_t$ are iid Gaussian random variables with mean 0 and covariance matrices $V$ ($q \times q$) and $W$ ($p \times p$), respectively. We also assume $v_t \perp w_{t'}$ for all $t$ and $t'$. $y_t$ ($q \times 1$) is the observed data at time $t$, $x_t$ ($p \times 1$) is the unknown state at time $t$, and $F$ ($q \times p$) and $G$ ($p \times p$) are matrices that could contain unknown parameters. Lastly, we define the distribution of the prior state $x_0$ by $x_0 \sim \mbox{N}(m_0, C_0)$, where $\mbox{N}(\mu, \Sigma)$ represents the normal distribution with mean $\mu$ and covariance $\Sigma$.

We can allow additional covariates $U_t$ into the observation equation \eqref{eqn:obs} by including a term $U_t \beta$, where $U_t$ is a $q$ by $d$ matrix and $\beta$ is a $d$ by $1$ vector. In addition, we can allow $F$ to change over time. Thus, our DLM is now written as the following:

\begin{align}
y_t &= U_t\beta + F_tx_t + v_t \label{eqn:obs2} \\
x_t &= Gx_{t-1} + w_t \label{eqn:state2}
\end{align}

\noindent Letting $\beta = 0$, $U_t = 0$ for all $t$, and $F_t = F$ independent of $t$, we have the DLM described by equations \eqref{eqn:obs} and \eqref{eqn:state}.

\subsection{Regression with AR(p) errors}

We will now consider a regression model with autoregressive error structure and separate observation and state noises. This specific model is a special case of the the DLM described in equations \eqref{eqn:obs2} and \eqref{eqn:state2}. In this case, $G$ has the form given by

\[G = \left(
\begin{array}{ccccc}
\phi_1 & 1 & 0 & \cdots & 0 \\
\phi_2 & 0 & 1 & \cdots & 0 \\
\vdots & \vdots & & \ddots & \vdots \\
\phi_{p-1} & 0 & \multicolumn{2}{c}{\cdots} & 1 \\
\phi_p & 0 & \multicolumn{2}{c}{\cdots} & 0
\end{array}
\right)\]

\noindent In addition, we let $W = \sigma^2_see'$ and $V = \sigma^2_m\tilde{V}$, where $e$ is a $p$-length vector with first element equal to 1 and the rest 0, $\sigma^2_s$ is the variance of the state (i.e. process) noise, and $\sigma^2_m$ is the variance of the observation (i.e. measurement) noise. We assume that the prior state $x_0$ is drawn from the stationary distribution given by $x_0 \sim \mbox{N}(0, \sigma^2_s\tilde{C_0})$, where \[\mbox{vec}({\tilde{C_0}}) = (I - G \otimes G)^{-1}\mbox{vec}(ee')\]

\section{Estimation}

Assume $\tilde{V}$, $U_t$, and $F_t$ known for all $t$. Let $\phi = (\phi_1,\ldots,\phi_p)'$, and let $\theta = (\beta', \phi', \sigma^2_m, \sigma^2_s)'$ be unknown. We now consider estimation of the joint posterior distribution of the states and unknown parameters using a Gibbs sampling algorithm. Define $y_{1:t} = (y_1,\ldots,y_t)'$ and $x_{0:t} = (x_0,x_1,\ldots,x_t)'$ to be the observed data and unobserved states, respectively, up until time $t$. We can express the joint likelihood $p(y_{1:t},x_{0:t},\theta)$ by the following general form:

\begin{equation}
p(y_{1:t},x_{0:t},\theta) = \prod_{k=1}^t\left[p(y_k|x_k,\theta)p(x_k|x_{k-1},\theta)\right]p(x_0|\theta)p(\theta) \label{eqn:lik}
\end{equation}

\noindent We use independent priors on the components of $\theta$, i.e. $p(\theta) = p(\beta)p(\phi)p(\sigma^2_m)p(\sigma^2_s)$, where

\begin{align*}
&\beta \sim \mbox{N}(b_0, B_0) \qquad \sigma^2_m \sim \mbox{IG}(a_{m_0}, b_{m_0})\\
&\phi \sim \mbox{N}_{S_\phi}(\phi_0, \Phi_0) \qquad \sigma^2_s \sim \mbox{IG}(a_{s_0}, b_{s_0})
\end{align*}

\noindent The hyperparameters $b_0$, $B_0$, $\phi_0$, $\Phi_0$, $a_{m_0}$, $b_{m_0}$, $a_{s_0}$, and $b_{s_0}$ are all assumed known. $\mbox{IG}(a,b)$ represents an inverse-gamma distribution with shape parameter $a$ and rate parameter $b$. $\mbox{N}_{\Omega}(\mu, \Sigma)$ represents the normal distribution truncated onto the set $\Omega$, with mean and covariance of its corresponding untruncated normal distribution given by $\mu$ and $\Sigma$, respectively. $S_{\phi}$ represents the set of all $\phi$ such that $x_t$ follows a stationary distribution.

To implement a Gibbs sampling algorithm that generates MCMC samples from the joint posterior distribution $p(x_{0:t},\theta|y_{1:t})$, we must draw from the following full conditional distributions given here in general and described in more detail in Section  \ref{sec:fullcond}:

\begin{align*}
p(\beta|y_{1:t},x_{0:t},\phi,\sigma^2_m,\sigma^2_s) &\propto p(y_{1:t},x_{0:t},\theta) \propto \prod_{k=1}^t\left[p(y_k|x_k,\beta,\sigma^2_m)\right]p(\beta) \\
p(\sigma^2_m|y_{1:t},x_{0:t},\beta,\phi,\sigma^2_s) &\propto p(y_{1:t},x_{0:t},\theta) \propto \prod_{k=1}^t\left[p(y_k|x_k,\beta,\sigma^2_m)\right]p(\sigma^2_m) \\
p(\phi|y_{1:t},x_{0:t},\beta,\sigma^2_m,\sigma^2_s) &\propto p(y_{1:t},x_{0:t},\theta) \propto \prod_{k=1}^t\left[p(x_k|x_{k-1},\phi,\sigma^2_s)\right]p(x_0|\phi,\sigma^2_s)p(\phi) \\
p(\sigma^2_s|y_{1:t},x_{0:t},\beta,\phi,\sigma^2_m) &\propto p(y_{1:t},x_{0:t},\theta) \propto \prod_{k=1}^t\left[p(x_k|x_{k-1},\phi,\sigma^2_s)\right]p(x_0|\phi,\sigma^2_s)p(\sigma^2_s) \\
p(x_{0:t}|y_{1:t},\theta) &\propto p(y_{1:t},x_{0:t},\theta) \propto \prod_{k=1}^t\left[p(y_k|x_k,\theta)p(x_k|x_{k-1},\theta)\right]p(x_0|\theta)
\end{align*}

\subsection{Full conditional distributions} \label{sec:fullcond}

We derive the following full conditional distributions for the unknown parameters:

\begin{itemize}
\item $\beta|y_{1:t},x_{0:t},\phi,\sigma^2_m,\sigma^2_s \sim \mbox{N}(b_t,B_t)$, where
\begin{align*}
b_t &= B_t\left(\frac{1}{\sigma^2_m} \sum^t_{k=1} U_k'\tilde{V}^{-1}(y_k - F_kx_k) + B_0^{-1}b_0\right) \\
B_t &= \left(\frac{1}{\sigma^2_m} \sum^t_{k=1} U_k'\tilde{V}^{-1}U_k + B_0^{-1}\right)^{-1}
\end{align*}
\item $\sigma^2_m|y_{1:t},x_{0:t},\beta,\phi,\sigma^2_s \sim \mbox{IG}(a_{m_t}, b_{m_t})$, where
\begin{align*}
a_{m_t} &= tq/2 + a_{m_0} \\
b_{m_t} &= \frac{1}{2}\sum^t_{k=1} \left((y_k - U_k\beta - F_kx_k)'\tilde{V}^{-1}(y_k - U_k\beta - F_kx_k)\right) + b_{m_0}
\end{align*}
\item $p(\phi|y_{1:t},x_{0:t},\beta,\sigma^2_m,\sigma^2_s) = \Psi(\phi)\mbox{N}_{S_\phi}(\phi; \phi_t,\Phi_t)$, where
\begin{align*}
\phi_t &= \Phi_t\left(\frac{1}{\sigma^2_s} \sum^t_{k=1} \tilde{X}_k'\tilde{W}^{-1}x_k + \Phi_0^{-1}\phi_0\right) \\
\Phi_t &= \left(\frac{1}{\sigma^2_s} \sum^t_{k=1} \tilde{X}_k'\tilde{V}^{-1}\tilde{X}_k + \Phi_0^{-1}\right)^{-1} \\
\tilde{X}_k &= \left(\begin{array}{ccccc}
x_{k-1,1} & x_{k-2,1} & \cdots & \cdots & x_{k-p,1} \\
0 & x_{k-1,1} & \cdots & \cdots & x_{k-(p-1),1} \\
\vdots & 0 & \ddots & & \vdots \\
\vdots & \vdots & \ddots & \ddots & \vdots \\
0 & 0 & \cdots & 0 & x_{k-1,1}
\end{array}
\right) \mbox{ with } x_k = \left(\begin{array}{c} x_{k,1} \\ x_{k,2} \\ \vdots \\ x_{k,p} \end{array}\right)\\
\Psi(\phi) &= \left|\tilde{C_0}\right|^{-1/2}\exp\left(-\frac{1}{2\sigma^2_s}x_0'\tilde{C_0}x_0\right)
\end{align*}
\noindent and $\mbox{N}_{\Omega}(.;\mu,\Sigma)$ represents the pdf of the normal distribution truncated onto the set $\Omega$, with mean and covariance of its corresponding untruncated normal distribution given by $\mu$ and $\Sigma$, respectively. If $p > 1$, the pre-sample states $\{x_{-1,1}, x_{-2,1}, \ldots, x_{-(p-1),1}\}$ need to be calculated in order to construct $\tilde{X}_k$ for $k = 1$. These can be obtained by the following recursive procedure: \\
\\
For each $k \in \{1,2,\ldots,p-1\}$:
\begin{enumerate}
\item Calculate $x_{-k,1} = x_{1-k,p} / \phi_p$. If $k = p - 1$, stop. Otherwise, proceed to Step \ref{step:recurse}.
\item For each $j \in \{p,p-1,\ldots,3\}$, calculate $x_{-k,j} = x_{1-k,j-1} - \phi_{j-1}x_{-k,1}$. \label{step:recurse}
\end{enumerate}
\item $\sigma^2_s|y_{1:t},x_{0:t},\beta,\phi,\sigma^2_m \sim \mbox{IG}(a_{s_t}, b_{s_t})$, where
\begin{align*}
a_{s_t} &= (p/2)(t+1) + a_{s_0} \\
b_{s_t} &= \frac{1}{2}\sum^t_{k=1} \left((x_k - Gx_{k-1})'\tilde{W}^{-1}(x_k - Gx_{k-1})\right) + \frac{1}{2}x_0'\tilde{C_0}x_0 + b_{s_0}
\end{align*}
\end{itemize}

\subsection{Gibbs sampler}

We generate samples from the joint posterior distribution $p(x_{0:t},\theta|y_{1:t})$ using the following MCMC algorithm:

\begin{enumerate}
\item Set $i = 0$ and draw initial values $x_{0:t}^{(i)} = \left(x_0^{(i)},x_1^{(i)},\ldots,x_t^{(i)}\right)'$ and $\theta^{(i)} = \left({\beta^{(i)}}', {\phi^{(i)}}', {\sigma^2_m}^{(i)}, {\sigma^2_s}^{(i)}\right)$. Set $i = 1$.
\item Sample $\beta^{(i)}|\cdots \sim \mbox{N}(b_n^{(i)}, B_n^{(i)})$, where
\begin{align*}
b_n^{(i)} &= B_n^{(i)}\left(\frac{1}{{\sigma^2_m}^{(i-1)}} \sum^t_{k=1} U_k'\tilde{V}^{-1}(y_k - F_kx_k^{(i-1)}) + B_0^{-1}b_0\right) \\
B_n^{(i)} &= \left(\frac{1}{{\sigma^2_m}^{(i-1)}} \sum^t_{k=1} U_k'\tilde{V}^{-1}U_k + B_0^{-1}\right)^{-1}
\end{align*}
\item Sample ${\sigma^2_m}^{(i)}|\cdots \sim \mbox{IG}(a_{m_t}^{(i)}, b_{m_t}^{(i)})$, where
\begin{align*}
a_{m_t}^{(i)} &= tq/2 + a_{m_0} \\
b_{m_t}^{(i)} &= \frac{1}{2}\sum^t_{k=1} \left((y_k - U_k\beta^{(i)} - F_kx_k^{(i-1)})'\tilde{V}^{-1}(y_k - U_k\beta^{(i)} - F_kx_k^{(i-1)})\right) + b_{m_0}
\end{align*}
\item Sample $\phi^{(i)}$ using the following Metropolis-Hastings step:
\begin{enumerate}
\item Draw $\phi^* \sim \mbox{N}(\phi_t^{(i)},\Phi_t^{(i)})$, where
\begin{align*}
\phi_t^{(i)} &= \Phi_t^{(i)}\left(\frac{1}{{\sigma^2_s}^{(i-1)}} \sum^t_{k=1} \tilde{X}_k'\tilde{W}^{-1}x_k^{(i)} + \Phi_0^{-1}\phi_0\right) \\
\Phi_t^{(i)} &= \left(\frac{1}{{\sigma^2_s}^{(i-1)}} \sum^t_{k=1} \tilde{X}_k'\tilde{V}^{-1}\tilde{X}_k + \Phi_0^{-1}\right)^{-1}
\end{align*} \label{step:phi}
\item If $\phi^* \in S_{\phi}$, go to Step \ref{step:mh}. Otherwise, go back to step \ref{step:phi}.
\item Calculate $R = \mbox{min}\left\{\frac{\Psi(\phi*)}{\Psi(\phi^{(i-1)})}, 1\right\}$ and draw $u \sim \mbox{Unif}[0,1]$. If $u < R$, set $\phi^{(i)} = \phi^*$. Otherwise, set $\phi^{(i)} = \phi^{(i-1)}$. \label{step:mh}
\end{enumerate}
\item Sample ${\sigma^2_s}^{(i)}|\cdots \sim \mbox{IG}(a_{s_t}^{(i)}, b_{s_t}^{(i)})$, where
\begin{align*}
a_{s_t}^{(i)} &= (p/2)(t+1) + a_{s_0} \\
b_{s_t}^{(i)} &= \frac{1}{2}\mbox{SS}_x + \frac{1}{2}{x_0^{(i-1)}}'\tilde{C_0}(\phi^{(i)})x_0^{(i-1)} + b_{s_0} \\
\mbox{SS}_x &= \sum^t_{k=1} \left((x_k^{(i-1)} - G(\phi^{(i)})x_{k-1}^{(i-1)})'\tilde{W}^{-1}(x_k^{(i-1)} - G(\phi^{(i)})x_{k-1}^{(i-1)})\right)
\end{align*}
%\noindent Here, $G(\phi^{(i)}$ and $\tilde{C_0}^{
\end{enumerate}

\clearpage

\bibliographystyle{plainnat}
\bibliography{jarad}

\end{document} 