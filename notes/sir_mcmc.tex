\documentclass{article}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}

\begin{document}

Here we describe estimation of the states and unknown parameters of a state-space model of an epidemic outbreak using MCMC.

\section{Model \label{sec:model}}

Let $x_t = (s_t,i_t)'$ denote the state of the epidemic at time $t$. We consider a compartmental model of disease transmission that is governed by three parameters:

\begin{itemize}
\item $\beta$, the contact rate of spread of disease,
\item $\gamma$, the recovery time from infection, and
\item $\nu$, the mixing intensity of the population.
\end{itemize}

\noindent $\beta$, $\gamma$, and $\nu$ are each restricted to be nonnegative. Define $\theta = (\beta,\gamma,\nu)'$ to be the vector of unknown parameters in our model, and let $P$ be the size of the population. Then, we describe the evolution of the epidemic from time $t$ to $t + 1$ by
\begin{equation}
x_{t+1}\left|x_t,\theta\right. \sim N_\Omega\left(x_{t+1};f(x_t,\theta),Q(\theta)\right) \label{eqn:state}
\end{equation}
\noindent where
\[
f(x_t,\theta) = \left(
\begin{array}{c}
s_t - \beta i_ts^{\nu}_t \phantom{- \gamma i_t}\,\, \\
i_t +  \beta i_ts^\nu_t - \gamma i_t
\end{array}
\right)
\qquad
Q(\theta) = \frac{\beta}{P^2} \left(
\begin{array}{ccccc}
1 & -1 \\
-1 & 1 + \gamma/\beta
\end{array}
\right)
\]

\noindent and $\Omega = \{(s_t,i_t): s_t \ge 0, i_t \ge 0, s_t + i_t \le 1\}$. $N_{\Omega}(.; \mu,\Sigma)$ represents the pdf of the truncated normal distribution onto the set $\Omega$ with mean and covariance matrix of its corresponding untruncated normal distribution given by $\mu$ and $\Sigma$, respectively.

We consider observed data that are positive real numbers related to counts of emergency room visits, prescription sales, or calls to a hotline, for example, and we can observe data from these different streams/sources asynchronously in time. That is, at any time $t$, we can observe data from any subset of the streams (or possibly none of them). Let $y_{l,t}$ represent data coming from stream $l$ at time $t$, where $l = 1,2,\ldots,L$ and $t = 1,2,\ldots,T$. We model the log of the observations (so that $y_{l,t}$ is restricted to be positive) by
\begin{equation}
\log y_{l,t} \sim N\left(\log y_{l,t};b_li_t^{\varsigma_l} + \eta_l,\sigma_l^2\right) \label{eqn:obs}
\end{equation}
where $b_l$, $\varsigma_l$, and $\sigma_l$ are nonnegative constants and $\eta_l$ is a real number that determines the baseline level of incoming syndromic data from stream $l$. We assume $b_l$, $\varsigma_l$, $\sigma_l$, and $\eta_l$ are known for all $l$.

\section{Bayesian estimation}

We specify the following priors on the initial state $x_0$ and unknown parameters $\theta$:

\begin{align*}
p(x_0) &= p(i_0,s_0) = N(i_0;0.002,0.005^2)\delta_{1 - i_0}(s_0) \\
p(\theta) &= p(\beta,\gamma)p(\nu) \\
p(\beta/\gamma) &= LN(R_0; 0.7520, 0.1768^2) \\
p(\gamma) &= LN(\gamma;-2.1764,0.1183^2) \\
p(\beta, \gamma) &\propto \frac{1}{\gamma} \exp\left(-\frac{1}{2} \left[\frac{1}{0.1768}(\log\frac{\beta}{\gamma} - 0.7520)^2 + \frac{1}{0.1183}(\log\frac{\beta}{\gamma} - [-2.1764])^2\right] \right) \\
p(\nu) &= LN(\nu;0.1055,0.0800^2)
\end{align*}

We can now derive full conditional distributions of the states and unknown parameters using Bayes' rule. We use the fact that the joint density of the observations, states, and unknown parameters can be calculated by

\begin{equation}
p(y_{1:T},x_{0:T},\theta) = \prod_{t = 1}^T \left\{p(y_t|x_t,\theta)p(x_t|x_{t-1},\theta)\right\}p(x_0,\theta) \label{eqn:joint}
\end{equation}

\noindent where $y_t = (y_{1,t},y_{2,t},\ldots,y_{L,t})'$ and

\begin{align*}
p(y_t|x_t) &= \prod_{l=1}^L \left(LN(y_{l,t};b_li_t^{\varsigma_l},\sigma_l^2)\right) \\
p(x_t|x_{t-1},\theta) &= p(s_t,i_t|s_{t-1},i_{t-1},\theta) = p(i_t|s_t,s_{t-1},i_{t-1},\theta)p(s_t|s_{t-1},i_{t-1},\theta) \\
&= N_{[0,1-s_t]}(i_t;i_{t-1}(1 - \gamma) + s_{t-1} - s_t, \gamma / P^2)N_{[0,1]}(s_t; s_{t-1} - \beta i_{t-1}s^{\nu}_{t-1}, \beta / P^2) \\
p(x_0,\theta) &= p(x_0)p(\beta, \gamma)p(\nu)
\end{align*}

\noindent In the above equations, $LN(.;\mu,\sigma^2)$ and $N(.;\mu,\sigma^2)$ represent the probability density functions of the lognormal and normal distributions, respectively, with mean $\mu$ and variance $\sigma^2$.

\section{Full conditionals} \label{sec:fullcond}
We can now express the full conditional distributions of the states $x_t$ for $t = 0, 1, \ldots, T$ and unknown parameters $\beta$, $\gamma$, and $\nu$ up to a proportionality constant by

\begin{align*}
p(x_0|\hdots) &\propto p(x_1|x_0,\theta)p(x_0) \\
p(x_t|\hdots) &\propto p(y_t|x_t)p(x_{t+1}|x_t,\theta)p(x_t|x_{t-1},\theta) \mbox{, for } t = 1,\ldots,T-1 \\
p(x_T|\hdots) &\propto p(y_T|x_T)p(x_T|x_{T-1},\theta) \\
p(\beta|\hdots) &\propto \prod_{t=1}^T \{p(x_t|x_{t-1},\theta)\}p(\beta, \gamma) \\
p(\gamma|\hdots) &\propto \prod_{t=1}^T \{p(x_t|x_{t-1},\theta)\}p(\beta, \gamma) \\
p(\nu|\hdots) &\propto \prod_{t=1}^T \{p(x_t|x_{t-1},\theta)\}p(\nu)
\end{align*}

\section{Gibbs Sampler and Metropolis-Hastings} \label{sec:gibbs}

To obtain draws from the joint posterior distribution $p(x_{0:T},\theta|y_{1:T})$, we use Gibbs sampling where each step draws from the full conditional distribution using a random-walk Metropolis-Hastings proposal distribution. The algorithm proceeds in the following manner:

\begin{enumerate}
\item Start with initial draws $x^{(0)}_{0:T} = (x^{(0)}_0, x^{(0)}_1, \ldots, x^{(0)}_T)'$ and $\theta^{(0)} = (\beta^{(0)}, \gamma^{(0)}, \nu^{(0)})'$. Set $j = 1$.
\item \label{step:gibbs} Sample the states, $x^{(j)}_k$, $k = 0,1,\ldots,T$, from their full conditional distributions. For $k = 1, 2, \ldots, T$,
    \begin{enumerate}[label=\alph*.,leftmargin=1.75\parindent]
    \item Draw $x^*_k \sim N(x^{(j-1)},\tau^2_{x_k})$.
    \item Calculate Metropolis ratio, $R$ by
    \[R = \qquad \left\{
    \begin{array}{cc}
    \frac{p(x^{(j-1)}_1|x^*_0,\theta^{(j-1)})p(x^*_0)}{p(x^{(j-1)}_1|x^{(j-1)}_0,\theta^{(j-1)})p(x^{(j-1)}_0)} & \mbox{, if } k = 0 \\
    \frac{p(y_k|x^*_k)p(x^{(j-1)}_{k+1}|x^*_k,\theta^{(j-1)})p(x^*_k|x^{(j)}_{k-1},\theta^{(j-1)})}{p(y_k|x^{(j-1)}_k)p(x^{(j-1)}_{k+1}|x^{(j-1)}_k,\theta^{(j-1)})p(x^{(j-1)}_k|x^{(j)}_{k-1},\theta^{(j-1)})} & \mbox{, if } 1 \le k \le T-1 \\
        \frac{p(y_T|x^*_T)p(x^*_T|x^{(j)}_{T-1},\theta^{(j-1)})}{p(y_T|x^{(j-1)}_T)p(x^{(j-1)}_T|x^{(j)}_{T-1},\theta^{(j-1)})}  & \mbox{, if } k = T
    \end{array}
    \right\}\].
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $x^{(j)}_k = x^*_k$. Otherwise, set $x^{(j)}_k = x^{(j-1)}_k$.
    \end{enumerate}
\item Sample $\beta^{(j)}$ from its full conditional conditional distribution.
    \begin{enumerate}[label=\alph*.,leftmargin=1.75\parindent]
    \item Draw $\beta^* \sim N(\beta^{(j-1)},\tau^2_{\beta})$.
    \item Calculate Metropolis ratio, $R$ by
    \[R = \frac{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^*,\gamma^{(j-1)},\nu^{(j-1)})\}p(\beta^*,\gamma)}{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\theta^{(j-1)})\}p(\beta^{(j-1)},\gamma)}\].
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\beta^{(j)} = \beta^*$. Otherwise, set $\beta^{(j)} = \beta^{(j-1)}$.
    \end{enumerate}
\item Sample $\gamma^{(j)}$ from its full conditional conditional distribution.
    \begin{enumerate}[label=\alph*.,leftmargin=1.75\parindent]
    \item Draw $\gamma^* \sim N(\gamma^{(j-1)},\tau^2_{\gamma})$.
    \item Calculate Metropolis ratio, $R$ by
    \[R = \frac{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^*,\nu^{(j-1)})\}p(\beta, \gamma^*)}{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j-1)},\nu^{(j-1)})\}p(\beta, \gamma^{(j-1)})}\].
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\gamma^{(j)} = \gamma^*$. Otherwise, set $\gamma^{(j)} = \gamma^{(j-1)}$.
    \end{enumerate}
\item Sample $\nu^{(j)}$ from its full conditional conditional distribution.
    \begin{enumerate}[label=\alph*.,leftmargin=1.75\parindent]
    \item Draw $\nu^* \sim N(\nu^{(j-1)},\tau^2_{\nu})$.
    \item Calculate Metropolis ratio, $R$ by
    \[R = \frac{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j)},\nu^*)\}p(\nu^*)}{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j)},\nu^{(j-1)})\}p(\nu^{(j-1)})}\].
    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\nu^{(j)} = \nu^*$. Otherwise, set $\nu^{(j)} = \nu^{(j-1)}$.
    \end{enumerate}
\item Set $j = j + 1$ and go back to step \ref{step:gibbs}
\end{enumerate}

\noindent The standard deviation of the random-walk proposal distributions, i.e. $\tau_{x_k}$ for $k = 0,1,\ldots,T$, $\tau_{\beta}$, $\tau_{\gamma}$, and $\tau_{\nu}$, are tuning parameters that are adjusted during the burn-in period of the MCMC. During burn-in, if the proposed value of a state or parameter at any given step of the Gibbs sampler is accepted, the corresponding tuning parameter is adjusted by multiplying by 1.1. If the proposed value is rejected, the tuning parameter is adjusted by dividing by 1.1.

\section{Slice Sampling}

Instead of using a random-walk Metropolis-Hastings algorithm, we could alternately obtain draws from the full conditional distributions in the Gibbs sampler using slice sampling. The idea behind slice sampling is, given a current sample $x$ and target distribution $f$, to incorporate an auxiliary variable $u$ that is sampled from a uniform distribution along $[0,f(x)]$ and generate a new sample $x^*$ by taking a uniform draw over the set $A = \{x: u < f(x)\}$. We implement the ``stepping-out'' slice sampler using the following algorithm:

\begin{enumerate}
\item Start with initial draws $x^{(0)}_{0:T} = (x^{(0)}_0, x^{(0)}_1, \ldots, x^{(0)}_T)'$ and $\theta^{(0)} = (\beta^{(0)}, \gamma^{(0)}, \nu^{(0)})'$. Set $j = 1$ and set values for the average slice interval $w$ and maximum number of steps for setting the interval, $m$.
\item \label{step:gibbs} Sample the states, $x^{(j)}_k$, $k = 0,1,\ldots,T$, from their full conditional distributions. For $k = 1, 2, \ldots, T$,
    \begin{enumerate}[label=\alph*.,leftmargin=1.75\parindent]
    \item Define the target density function $f$ by
    \[f(x_k) = \qquad \left\{
    \begin{array}{cc}
    p(x^{(j-1)}_{k+1}|x_k,\theta^{(j-1)})p(x_k) & \mbox{, if } k = 0 \\
    p(y_k|x_k)p(x^{(j-1)}_{k+1}|x_k,\theta^{(j-1)})p(x_k|x^{(j)}_{k-1},\theta^{(j-1)}) & \mbox{, if } 1 \le k \le T-1 \\
    p(y_k|x_k)p(x_k|x^{(j)}_{k-1},\theta^{(j-1)}) & \mbox{, if } k = T
    \end{array}
    \right\}\]
    \item Draw $u \sim \mbox{Unif}[0,f(x^{(j-1)}_k)]$. This defines a horizontal slice of the target density.
    \item Randomly set an interval around the current value $x^{(j-1)}_k$ by drawing $u_L \sim \mbox{Unif}[0,w]$ and setting lower and upper bounds $L$ and $R$: \[L = x^{(j-1)}_k - u_L \qquad R = x^{(j-1)}_k + w\].
    \item Attempt to extend the interval until the target density at the bounds of the interval are below the slice.
      \begin{enumerate}
      \item Draw $u_s \sim \mbox{Unif}[0,1]$ and set \[J = \left \lfloor{m\times u_s}\right \rfloor \qquad K = m - 1 - J \].
      \item {\tt while(} $u < f(L)$ and $J > 0${\tt)} Set $L = L - w$ and $J = J - 1$.
      \item {\tt while(} $u < f(R)$ and $K > 0${\tt)} Set $R = R + w$ and $K = K - 1$.
      \end{enumerate}
    \item Sample and shrink the interval until the density at the sampled value is above the slice.
      \begin{enumerate}
      \item Draw $x^{(j)}_k \sim \mbox{Unif}[L,R]$. \label{step:samp}
      \item {\tt if(}$u \ge f(x^{(j)}_k)${\tt)\{} \\
       {\tt if(} $f(x^{(j)}_k) > f(x^{(j-1)}_k)${\tt)} Set $R = x^{(j)}_k$. \\
       {\tt if(} $f(x^{(j)}_k) < f(x^{(j-1)}_k)${\tt)} Set $L = x^{(j)}_k$. \\
       Return to step \ref{step:samp} \\
       {\tt \}}
      \end{enumerate}
%    \item Draw $x^*_k \sim N(x^{(j-1)},\tau^2_{x_k})$.
%    \item Calculate Metropolis ratio, $R$ by
%    \[R = \qquad \left\{
%    \begin{array}{cc}
%    \frac{p(x^{(j-1)}_1|x^*_0,\theta^{(j-1)})p(x^*_0)}{p(x^{(j-1)}_1|x^{(j-1)}_0,\theta^{(j-1)})p(x^{(j-1)}_0)} & \mbox{, if } k = 0 \\
%    \frac{p(y_k|x^*_k)p(x^{(j-1)}_{k+1}|x^*_k,\theta^{(j-1)})p(x^*_k|x^{(j)}_{k-1},\theta^{(j-1)})}{p(y_k|x^{(j-1)}_k)p(x^{(j-1)}_{k+1}|x^{(j-1)}_k,\theta^{(j-1)})p(x^{(j-1)}_k|x^{(j)}_{k-1},\theta^{(j-1)})} & \mbox{, if } 1 \le k \le T-1 \\
%        \frac{p(y_T|x^*_T)p(x^*_T|x^{(j)}_{T-1},\theta^{(j-1)})}{p(y_T|x^{(j-1)}_T)p(x^{(j-1)}_T|x^{(j)}_{T-1},\theta^{(j-1)})}  & \mbox{, if } k = T
%    \end{array}
%    \right\}\].
%    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $x^{(j)}_k = x^*_k$. Otherwise, set $x^{(j)}_k = x^{(j-1)}_k$.
%    \end{enumerate}
%\item Sample $\beta^{(j)}$ from its full conditional conditional distribution.
%    \begin{enumerate}[label=\alph*.,leftmargin=1.75\parindent]
%    \item Draw $\beta^* \sim N(\beta^{(j-1)},\tau^2_{\beta})$.
%    \item Calculate Metropolis ratio, $R$ by
%    \[R = \frac{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^*,\gamma^{(j-1)},\nu^{(j-1)})\}p(\beta^*)}{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\theta^{(j-1)})\}p(\beta^{(j-1)})}\].
%    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\beta^{(j)} = \beta^*$. Otherwise, set $\beta^{(j)} = \beta^{(j-1)}$.
%    \end{enumerate}
%\item Sample $\gamma^{(j)}$ from its full conditional conditional distribution.
%    \begin{enumerate}[label=\alph*.,leftmargin=1.75\parindent]
%    \item Draw $\gamma^* \sim N(\gamma^{(j-1)},\tau^2_{\gamma})$.
%    \item Calculate Metropolis ratio, $R$ by
%    \[R = \frac{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^*,\nu^{(j-1)})\}p(\gamma^*)}{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j-1)},\nu^{(j-1)})\}p(\gamma^{(j-1)})}\].
%    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\gamma^{(j)} = \gamma^*$. Otherwise, set $\gamma^{(j)} = \gamma^{(j-1)}$.
%    \end{enumerate}
%\item Sample $\nu^{(j)}$ from its full conditional conditional distribution.
%    \begin{enumerate}[label=\alph*.,leftmargin=1.75\parindent]
%    \item Draw $\nu^* \sim N(\nu^{(j-1)},\tau^2_{\nu})$.
%    \item Calculate Metropolis ratio, $R$ by
%    \[R = \frac{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j)},\nu^*)\}p(\nu^*)}{\prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j)},\gamma^{(j)},\nu^{(j-1)})\}p(\nu^{(j-1)})}\].
%    \item Draw $u \sim \mbox{Unif}[0,1]$. If $u < \min\{1, R\}$, set $\nu^{(j)} = \nu^*$. Otherwise, set $\nu^{(j)} = \nu^{(j-1)}$.
    \end{enumerate}
\item Sample $\beta^{(j)}$ from its full conditional conditional distribution. Apply the same procedure used for sampling the states $x^{(j)}_k$ in step \ref{step:gibbs} to sampling $\beta^{(j)}$ with target density $f$ defined by
    \[f(\beta) = \prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta,\gamma^{(j-1)},\nu^{(j-1)})\}p(\beta)\].
\item Sample $\gamma^{(j)}$ from its full conditional conditional distribution. Apply the same procedure used for sampling the states $x^{(j)}_k$ in step \ref{step:gibbs} to sampling $\gamma^{(j)}$ with target density $f$ defined by
    \[f(\gamma) = \prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j-1)},\gamma,\nu^{(j-1)})\}p(\gamma)\].
\item Sample $\nu^{(j)}$ from its full conditional conditional distribution. Apply the same procedure used for sampling the states $x^{(j)}_k$ in step \ref{step:gibbs} to sampling $\nu^{(j)}$ with target density $f$ defined by
    \[f(\nu) = \prod_{t=1}^T \{p(x^{(j)}_t|x^{(j)}_{t-1},\beta^{(j-1)},\gamma^{(j-1)},\nu)\}p(\nu)\].
\item Set $j = j + 1$ and go back to step \ref{step:gibbs}
\end{enumerate}

\end{document} 