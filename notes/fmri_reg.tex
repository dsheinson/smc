\documentclass{article}

\usepackage{amsmath}
\usepackage{natbib}
\usepackage{enumitem}
\usepackage{graphicx}

\graphicspath{
{/Users/Danny/Documents/"UCSB - Research"/smc/graphs/mcmcTest/},
{/Users/Danny/Documents/"UCSB - Research"/smc/graphs/twoEvent/}
}

\begin{document}

\section{Regression with AR(p) errors} \label{sec:model}

Consider the following regression model for a time series $y_t$, $t = 1,2,\ldots,T$, from a single voxel of the brain in an fMRI experiment:

\begin{equation}
y_t = x_t'\beta + w_t \label{eqn:reg}
\end{equation}

\noindent where $\beta = (\beta_0,\beta_1,\ldots,\beta_{d-1})$ is a vector of unknown coefficients, $x_t$ is a $d$-length vector of covariates at time $t$, and $w_t$ follows a $p$-order autoregressive process given by

\begin{equation}
w_t = \rho_1 w_{t-1} + \rho_2 w_{t-2} + \cdots + \rho_p w_{t-p} + \epsilon_t \label{eqn:ar}
\end{equation}

\noindent with $\epsilon_t \stackrel{\mbox{iid}}{\sim} \mbox{N}(0, \sigma^2)$. We can express equation \eqref{eqn:ar} in terms of a polynomial in the backshift operator $L$ as \[\rho(L)w_t = \epsilon_t\] where $\rho_p \ne 0$ and $\rho(L) = 1 - \rho_1L - \rho_2L^2 - \cdots - \rho_pL^p$. The model can be written in matrix form by

\begin{equation}
y = X\beta + w \label{eqn:matrix}
\end{equation}

\noindent with $y = (y_1,y_2,\ldots,y_T)'$, $X = (x_1|x_2|\cdots|x_T)'$, and $w = (w_1,w_2,\ldots,w_T)'$.

We make the following assumptions about the model:
\begin{enumerate}
\item Stationarity: All roots of $\rho(L)$ lie outside the unit circle, and the first $p$ data points $y_{1:p} = (y_1,\ldots,y_p)'$ following the stationary distribution given by \[y_{1:p}|\beta,\rho,\sigma^2 \sim \mbox{N}(X_{1:p}\beta,\sigma^2\Sigma_p)\] where $X_{1:p} = (x_1|x_2|\cdots|x_p)'$ is the first $p$ rows of $X$, $\Sigma_p = \Omega'\Sigma_p\Omega + e_1(p)e_1(p)'$ (or equivalently $\mbox{vec}(\Sigma_p) = (I - \Omega \otimes \Omega)^{-1}\mbox{vec}(e_1(p)e_1(p))$), \[\Omega = \left(\begin{array}{ccccc} \rho_1 & 1 & 0 & \cdots & 0 \\
    \rho_2 & 0 & 1 & \cdots & 0 \\
    \vdots & \vdots & & \ddots & \vdots \\
    \rho_{p-1} & 0 & \multicolumn{2}{c}{\cdots} & 1 \\
    \rho_p & 0 & \multicolumn{2}{c}{\cdots} & 0 \end{array}\right),\] and $e_1(p) = (1,0,\ldots,0)'$ is a $p$-length vector.
\item Prior distributions: $p(\beta,\rho,\sigma^2) = p(\beta)p(\rho)p(\sigma^2)$, where
\begin{align*}
\beta &\sim \mbox{N}(b_0,B_0) \\
\rho &\sim \mbox{N}(r_0,R_0) \\
\sigma^2 &\sim \mbox{IG}(v_0,d_0)
\end{align*}
\end{enumerate}

\section{Posterior} \label{sec:fullcond}

Letting $\theta = (\beta,\rho,\sigma^2)'$, the posterior density is \[p(\theta|y) \propto p(y|\theta)p(\theta)\]

\subsection{Full conditional for $\beta$ and $\sigma^2$} \label{sec:fullcondbeta}

First, consider the first $p$ data points $y_{1:p}$. Let $Q_1$ be the Cholesky decomposition of $\Sigma_p$, i.e. $Q_1Q_1' = \Sigma_p$. Then, define $\tilde{y}_1 = Q_1^{-1}y_{1:p}$ and $\tilde{X}_1 = Q_1^{-1}X_{1:p}$. Then, it follows that $\tilde{y}_1 \sim \mbox{N}(\tilde{X}_1\beta,\sigma^2I_p)$.

Next, consider the last $T - p$ data points $y_{p+1:T}$. Define $\tilde{y}_2 = \rho(L)y_{p+1:T}$ and $\tilde{X}_2 = (\tilde{x}_{p+1}|\tilde{x}_{p+2}|\cdots|\tilde{x}_T)'$ where $\tilde{x}_t = \rho(L)x_t$. Noting that applying $\rho(L)$ to $y_t$ (for $t \ge p+1$) yields \[\rho(L)y_t = \rho(L)x_t'\beta + \rho(L)w_t = \tilde{x}_t + \epsilon_t,\] it follows that $\tilde{y}_2 \sim \mbox{N}(\tilde{X}_2\beta, \sigma^2I_{T-p})$.

Finally, letting $\tilde{y} = (\tilde{y}_1',\tilde{y}_2')'$ and $\tilde{X} = (\tilde{X}_1'|\tilde{X}_2')'$, it follows that

\begin{equation}
\tilde{y} \sim \mbox{N}(\tilde{X}\beta,\sigma^2I_T) \label{eqn:tilde}
\end{equation}

\noindent This leads to the following results:

\begin{align}
\beta | \rho, \sigma^2, y_{1:T} &\sim \mbox{N}(b_T,B_T) \label{eqn:fullcondbeta} \\
\sigma^2 | \beta, \rho, y_{1:T} &\sim \mbox{IG}(v_t,d_t) \label{eqn:fullcondsigma2s}
\end{align}

\noindent where

\begin{align*}
b_T &= B_T(\frac{1}{\sigma^2}\tilde{X}'\tilde{y} + B_0^{-1}b_0) \\
B_T &= (\frac{1}{\sigma^2}\tilde{X}'\tilde{X} + B_0^{-1})^{-1} \\
v_T &= T/2 + v_0 \\
d_T &= \frac{1}{2}\mbox{SS}_y + d_0
\end{align*}

\noindent and $\mbox{SS}_y = (\tilde{y} - \tilde{X}\beta)'(\tilde{y} - \tilde{X}\beta)$

\subsection{Full conditional for $\rho$}

Let $e = (e_{p+1},e_{p+2},\ldots,e_T)'$ be a $T-p$ length vector with $e_t = y_t - x_t'\beta$ for $t \ge p+1$. Then, let $E$ be a $T-p \times p$ matrix with $t$th row given by $(e_{t-1+p},e_{t-2+p}\ldots,e_t)$, i.e. \[E = \left(\begin{array}{cccc}
e_p & e_{p-1} & \cdots & e_1 \\
e_{p+1} & e_p & \cdots & e_2 \\
\vdots & \vdots & & \vdots \\
e_{T-1} & e_{T-2} & \cdots & e_{T-p}
\end{array}\right).\] The full conditional for $\rho$ can then be expressed by

\begin{equation}
\rho | \beta, \sigma^2, y_{1:T} \sim \Psi(\rho) \times \mbox{N}_{S_{\rho}}(r_T,R_T) \label{eqn:fullcondrho}
\end{equation}

\noindent where

\begin{align*}
r_T &= R_T(\frac{1}{\sigma^2}E'e + R_0^{-1}r_0) \\
R_T &= (\frac{1}{\sigma^2}E'E + R_0^{-1})^{-1} \\
\Psi(\rho) &= |\Sigma_p|^{-1/2} \exp\left[-\frac{1}{\sigma^2}(y_{1:p} - X_{1:p}\beta)'\Sigma_p^{-1}(y_{1:p} - X_{1:p}\beta)\right],
\end{align*}

\noindent $\mbox{N}_{\Theta}(\mu,\Sigma)$ represents the normal distribution truncated onto the set $\Theta$ with mean and covariance of the corresponding untruncated normal distribution represented by $\mu$ and $\Sigma$, respectively, and $S_{\rho}$ represents the set of all $\rho$ such that the roots of $\rho(L)$ lie outside the unit circle.

Given a current value $\rho^{(i-1)}$ at iteration $i-1$ of an MCMC algorithm, we can sample from the full conditional for $\rho$ at iteration $i$ using the following Metropolis-Hastings step:
\begin{enumerate}
\item Draw $\rho^* \sim \mbox{N}(r_T,R_T)$ \label{step:rho}
\item If $\rho^* \in S_{\rho}$, go to Step \ref{step:mh}. Otherwise, go back to step \ref{step:rho}.
\item Calculate $R = \mbox{min}\left\{\frac{\Psi(\rho*)}{\Psi(\rho^{(i-1)})}, 1\right\}$ and draw $u \sim \mbox{Unif}[0,1]$. If $u < R$, set $\rho^{(i)} = \rho^*$. Otherwise, set $\rho^{(i)} = \rho^{(i-1)}$. \label{step:mh}
\end{enumerate}

\section{Examples - Regression with AR(1) and AR(2) errors} \label{sec:ex}

A time series was generated from a regression model with AR(1) errors, i.e. a model of the form given by equations \eqref{eqn:reg} and \eqref{eqn:ar} with parameter values set to
\begin{align*}
\beta = (\beta_0,\beta_1)' &\quad \beta_0 = 900 &\quad \beta_1 = 5 \\
\phi = (\phi_1) = 0.8 &\quad \sigma_s^2 = 1
\end{align*}
Another time series was simulated from a regression model with AR(2) errors using the same parameter values as for the AR(1) error model, except with $\phi = (\phi_1,\phi_2)' = (0.6, -0.1)'$.

Three MCMC chains were run on each data set using a Gibbs sampling algorithm that samples from the full conditional distributions described in Section \ref{sec:fullcond}. Each chain was run for 11000 iterations with a burn-in period of 1000. Samples from every 10th iteration of the Gibbs sampler were saved, yielding the following marginal traceplots for the fixed parameters shown in Figure \ref{fig:tracetheta1} and \ref{fig:tracetheta2} for the models with AR(1) errors and AR(2) errors, respectively. Maximum likelihood estimates shown in the figures were obtained using the {\tt arima()} function in R.

\begin{figure}[ht]
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_reg_mcmc_test-20-6-1-M010-2-3-11000-1000-10-traceplots-beta}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_reg_mcmc_test-20-6-1-M010-2-3-11000-1000-10-traceplots-phi-1}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_reg_mcmc_test-20-6-1-M010-2-3-11000-1000-10-traceplots-sigma2s-1}
\end{minipage}
\caption{Marginal traceplots for unknown parameters of a regression model with AR(1) errors for three MCMC chains (colors). Gray horizontal lines correspond to MLEs and black horizontal lines correspond to true values used for simulation with actual values printed along the right side of the plot panel.} \label{fig:tracetheta1}
\end{figure}

\begin{figure}[ht]
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_reg_mcmc_test-20-6-1-M020-2-3-11000-1000-10-traceplots-beta}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_reg_mcmc_test-20-6-1-M020-2-3-11000-1000-10-traceplots-phi-1}
\end{minipage}
\begin{minipage}{0.5\linewidth}
\includegraphics[width=1.0\textwidth]{fmri_reg_mcmc_test-20-6-1-M020-2-3-11000-1000-10-traceplots-sigma2s-1}
\end{minipage}
\caption{Marginal traceplots for unknown parameters of a regression model with AR(2) errors for three MCMC chains (colors). Gray horizontal lines correspond to MLEs and black horizontal lines correspond to true values used for simulation with actual values printed along the right side of the plot panel.} \label{fig:tracetheta2}
\end{figure}

\end{document} 